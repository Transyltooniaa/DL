{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a96c85",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning for LLM Unlearning - Yelp Dataset (Local Machine)\n",
    "\n",
    "This notebook demonstrates how to fine-tune a large language model using QLoRA (Quantized Low-Rank Adaptation) for sentiment analysis tasks with unlearning capabilities using the **Yelp Polarity dataset**.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses pre-split Yelp dataset from HuggingFace (by research paper authors)\n",
    "- Dataset already has forget/retain splits created via k-means clustering\n",
    "- Optimized for local machine execution\n",
    "\n",
    "## Setup Steps:\n",
    "1. Install required dependencies\n",
    "2. Import libraries and define utilities\n",
    "3. Load pre-split dataset from HuggingFace\n",
    "4. Load model and evaluate base model (0-shot)\n",
    "5. Train the model on the dataset\n",
    "6. Evaluate fine-tuned model\n",
    "7. Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686125f4",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install all required packages including sentence-transformers for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - uncomment if needed\n",
    "# !pip install -q datasets evaluate peft scikit-learn\n",
    "# !pip install -q torch torchinfo transformers trl\n",
    "# !pip install -q bitsandbytes wandb\n",
    "# !pip install -q accelerate\n",
    "\n",
    "print(\"âœ… Dependencies should be installed. If not, uncomment the pip install commands above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132d201",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fded3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, TrainerState, TrainerControl, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, TrainerCallback, Trainer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958abd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"âœ… GPU cache cleared\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No CUDA available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ace47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA environment for better error reporting\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"âœ… CUDA_LAUNCH_BLOCKING enabled for better error reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a171ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch CUDA compatibility\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Try a simple CUDA operation\n",
    "    try:\n",
    "        x = torch.randn(10, 10).cuda()\n",
    "        print(\"âœ… CUDA test operation successful!\")\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CUDA test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efa8ab",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions\n",
    "\n",
    "These functions are adapted from `utils.py` with Yelp dataset support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f940a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(dataset):\n",
    "    \"\"\"Get the HuggingFace dataset path based on dataset name.\"\"\"\n",
    "    if dataset.lower() == \"sst2\":\n",
    "        data_path = \"karuna-bhaila/Unlearning_SST2v3\"\n",
    "    elif dataset.lower() == 'yelp':\n",
    "        data_path = \"karuna-bhaila/Unlearning_Yelp_Polarity\"\n",
    "    else:\n",
    "        # define dataset with the following splits:\n",
    "        # train_retain, train_forget, test_retain, test_forget\n",
    "        raise NotImplementedError(f\"Dataset {dataset} not supported\")\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Preprocess logits for metric computation.\"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # argmax to get the token ids\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load('precision')\n",
    "    recall_metric = evaluate.load('recall')\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = logits[:, :-1]\n",
    "    labels = labels[:, 1:]\n",
    "\n",
    "    check_labels = labels != -100\n",
    "\n",
    "    last_token_predictions = []\n",
    "    last_token_labels = []\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        last_token_predictions.append(predictions[idx][check_labels[idx]])\n",
    "        last_token_labels.append(labels[idx][check_labels[idx]])\n",
    "\n",
    "    f1 = f1_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='weighted')[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=last_token_predictions, references=last_token_labels)[\"accuracy\"]\n",
    "    precision = precision_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='micro')['precision']\n",
    "    recall = recall_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='micro')['recall']\n",
    "\n",
    "    return {\"f1-score\": f1, 'accuracy': accuracy, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for additional evaluation during training.\"\"\"\n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset['train_retain'],\n",
    "                                   metric_key_prefix=\"eval_train_retrain\")\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset['train_forget'],\n",
    "                                   metric_key_prefix=\"eval_train_forget\")\n",
    "            return control_copy\n",
    "\n",
    "print(\"Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d3902",
   "metadata": {},
   "source": [
    "## 4. Define Model and Dataset Functions\n",
    "\n",
    "These functions handle model initialization and dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e25228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(model_checkpoints, rank=4, alpha=16, lora_dropout=0.1, bias='none'):\n",
    "    \"\"\"Initialize model with QLoRA configuration.\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_checkpoints,\n",
    "        device_map=\"auto\",\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Configure LoRA with proper target modules and NO embedding/head training\n",
    "    if model_checkpoints == 'mistralai/Mistral-7B-v0.1' or model_checkpoints == 'meta-llama/Llama-2-7b-hf':\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=bias,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "            ],\n",
    "            inference_mode=False,  # Training mode\n",
    "            modules_to_save=None,  # Don't train embeddings or LM head\n",
    "        )\n",
    "    else:\n",
    "        # For OPT models, explicitly target attention layers only\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=bias,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],  # Explicit target modules for OPT\n",
    "            inference_mode=False,  # Training mode\n",
    "            modules_to_save=None,  # Don't train embeddings or LM head\n",
    "        )\n",
    "\n",
    "    return model, tokenizer, peft_config\n",
    "\n",
    "\n",
    "def get_unlearn_dataset(\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        add_prefix_space=True,\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "):\n",
    "    \"\"\"Load and prepare the unlearning dataset.\"\"\"\n",
    "    # Format: prompt and completion for completion-only loss\n",
    "    # The model will only compute loss on the completion part\n",
    "    def _preprocessing_sentiment(examples):\n",
    "        prompt = f\"\"\"### Text: {examples['text']}\\n\\n### Question: What is the sentiment of the given text?\\n\\n### Sentiment:\"\"\"\n",
    "        completion = f\"\"\" {examples['label_text']}\"\"\"\n",
    "        # Combine for SFTTrainer with completion_only_loss\n",
    "        return {\"text\": prompt + completion}\n",
    "\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "    data = data.map(_preprocessing_sentiment, batched=False)\n",
    "    data = data.remove_columns(['label', 'label_text'])\n",
    "    data.set_format(\"torch\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "print(\"Model and dataset functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee0adb",
   "metadata": {},
   "source": [
    "## 7. Configure Weights & Biases (Optional)\n",
    "\n",
    "If you want to track your training with W&B, login here. Otherwise, disable W&B tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# Disable W&B tracking (uncomment wandb.login() to enable)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# To enable W&B tracking:\n",
    "# wandb.login()\n",
    "# del os.environ[\"WANDB_DISABLED\"]\n",
    "\n",
    "print(\"W&B configuration set!\")\n",
    "print(f\"W&B tracking: {'disabled' if os.environ.get('WANDB_DISABLED') == 'true' else 'enabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c31cd",
   "metadata": {},
   "source": [
    "## 5. Configure Training Parameters\n",
    "\n",
    "Modify these parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9957fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arguments namespace (similar to command-line arguments)\n",
    "args = Namespace(\n",
    "    # Dataset configuration\n",
    "    dataset=\"yelp\",  # Changed from 'sst2' to 'yelp'\n",
    "\n",
    "    # Model configuration\n",
    "    model_name=\"facebook/opt-1.3b\",  # OPT-1.3B model\n",
    "    # Alternative models:\n",
    "    # - \"meta-llama/Llama-2-7b-hf\"\n",
    "    # - \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "    # Output configuration\n",
    "    output_path=None,  # Will be auto-generated if None\n",
    "\n",
    "    # Training hyperparameters\n",
    "    max_length=1024,\n",
    "    lr=1e-4,\n",
    "    train_batch_size=45,  # Adjust based on your GPU memory\n",
    "    eval_batch_size=45,\n",
    "    num_epochs=5,\n",
    "    weight_decay=0.001,\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_rank=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    lora_bias='none',  # Options: 'lora_only', 'none', 'all'\n",
    "\n",
    "    # Model-specific settings\n",
    "    set_pad_id=False,  # Set to True for Mistral-7B\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {args.dataset}\")\n",
    "print(f\"  Model: {args.model_name}\")\n",
    "print(f\"  Learning Rate: {args.lr}\")\n",
    "print(f\"  Batch Size: {args.train_batch_size}\")\n",
    "print(f\"  Epochs: {args.num_epochs}\")\n",
    "print(f\"  LoRA Rank: {args.lora_rank}\")\n",
    "print(f\"  LoRA Alpha: {args.lora_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13705feb",
   "metadata": {},
   "source": [
    "## 6. Setup Output Directory and W&B Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine model name for output directory\n",
    "if 'llama-2-7b' in args.model_name.lower():\n",
    "    model_name = 'llama-2-7b-hf'\n",
    "elif 'llama-2-13b' in args.model_name.lower():\n",
    "    model_name = 'llama-2-13b-hf'\n",
    "elif 'opt-1.3b' in args.model_name.lower():\n",
    "    model_name = 'opt-1.3b'\n",
    "elif 'mistral' in args.model_name.lower():\n",
    "    model_name = 'mistral-7b'\n",
    "else:\n",
    "    model_name = 'custom-model'\n",
    "\n",
    "# Setup W&B project (if enabled)\n",
    "if os.environ.get(\"WANDB_DISABLED\") != \"true\":\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"all\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = f'qlora_{model_name.lower()}_{args.dataset.lower()}'\n",
    "\n",
    "# Get data path for Yelp\n",
    "data_path = get_data_path(args.dataset)\n",
    "\n",
    "# Setup output directory\n",
    "if args.output_path is None:\n",
    "    args.output_path = f'qlora_checkpoints/{model_name.lower()}-qlora-{args.dataset.lower()}'\n",
    "\n",
    "os.makedirs(args.output_path, exist_ok=True)\n",
    "\n",
    "# Save arguments\n",
    "with open(os.path.join(args.output_path, 'arguments.txt'), 'w') as f:\n",
    "    for k, v in args.__dict__.items():\n",
    "        f.write(f'{k}: {v}\\n')\n",
    "\n",
    "print(f\"Output directory: {args.output_path}\")\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eeb44a",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Tokenizer\n",
    "\n",
    "**Note:** For local machine, HuggingFace token may be required for some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Hugging Face Hub if needed for gated models\n",
    "# from huggingface_hub import login\n",
    "# hf_token = \"your_hf_token_here\"  # Replace with your token\n",
    "# login(hf_token)\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model, tokenizer, lora_config = get_lora_model(\n",
    "    args.model_name,\n",
    "    rank=args.lora_rank,\n",
    "    alpha=args.lora_alpha,\n",
    "    lora_dropout=args.lora_dropout,\n",
    "    bias=args.lora_bias\n",
    ")\n",
    "\n",
    "# CRITICAL: Explicitly freeze ALL base model parameters before applying LoRA\n",
    "# This prevents SFTTrainer from making embeddings/layer norms trainable\n",
    "print(\"\\nðŸ”’ Freezing base model parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False  # Freeze everything first\n",
    "\n",
    "print(\"âœ… All base model parameters frozen!\")\n",
    "\n",
    "# Now apply LoRA using get_peft_model - this will ONLY make LoRA params trainable\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"âœ… LoRA adapters added!\")\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(\"\\nModel Summary:\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79717e68",
   "metadata": {},
   "source": [
    "## 8. Load Pre-Split Yelp Dataset from HuggingFace\n",
    "\n",
    "Load the Yelp dataset that has already been split into train/test retain/forget by the research paper authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Yelp dataset...\")\n",
    "\n",
    "dataset = get_unlearn_dataset(\n",
    "    data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=args.max_length,\n",
    "    add_prefix_space=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "print(\"\\nYelp Dataset splits:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"  {split}: {len(dataset[split])} examples\")\n",
    "\n",
    "# Show a sample from each split\n",
    "print(\"\\nðŸ“ Sample from each split:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"\\n{split} example:\")\n",
    "    print(dataset[split]['text'][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a164c6",
   "metadata": {},
   "source": [
    "## 9. Verify LoRA Configuration\n",
    "\n",
    "**IMPORTANT:** Check that only LoRA adapters are trainable, not the full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LoRA parameters - should be ~1-4M trainable params, not 200M+!\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    trainable_percentage = 100 * trainable_params / all_param\n",
    "\n",
    "    print(f\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(f\"â•‘          LoRA Parameter Verification         â•‘\")\n",
    "    print(f\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "    print(f\"â•‘ Total params:      {all_param:>15,} â•‘\")\n",
    "    print(f\"â•‘ Trainable params:  {trainable_params:>15,} â•‘\")\n",
    "    print(f\"â•‘ Non-trainable:     {all_param - trainable_params:>15,} â•‘\")\n",
    "    print(f\"â•‘ Trainable %:       {trainable_percentage:>14.4f}% â•‘\")\n",
    "    print(f\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "    # Validation checks\n",
    "    if trainable_percentage > 5.0:\n",
    "        print(\"\\nâš ï¸  WARNING: Trainable params > 5%! This is NOT typical for LoRA!\")\n",
    "        print(\"   Expected: 0.1-1% for LoRA (rank 8-64)\")\n",
    "        print(\"   Check that embeddings/LM head are NOT being trained.\")\n",
    "    elif trainable_percentage < 0.05:\n",
    "        print(\"\\nâš ï¸  WARNING: Trainable params < 0.05%! This might be too few.\")\n",
    "        print(\"   Consider increasing LoRA rank or checking target_modules.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Parameter count looks good for LoRA training!\")\n",
    "\n",
    "    return trainable_params, all_param\n",
    "\n",
    "# Print detailed parameter info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "trainable, total = print_trainable_parameters(model)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print which modules are trainable\n",
    "print(\"\\nðŸ“‹ Trainable modules:\")\n",
    "trainable_modules = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_modules.append(name)\n",
    "\n",
    "if len(trainable_modules) <= 50:  # Only print if reasonable number\n",
    "    for name in trainable_modules:\n",
    "        print(f\"  âœ“ {name}\")\n",
    "else:\n",
    "    print(f\"  Total: {len(trainable_modules)} trainable modules\")\n",
    "    print(\"  Sample (first 10):\")\n",
    "    for name in trainable_modules[:10]:\n",
    "        print(f\"  âœ“ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating base model before fine-tuning...\")\n",
    "print(\"This will give us a baseline to compare against.\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_base_model_simple(model, dataset, tokenizer, batch_size=8):\n",
    "    \"\"\"\n",
    "    Simple evaluation that computes accuracy by checking if the model\n",
    "    generates the correct sentiment token.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Map sentiment labels to expected tokens\n",
    "    sentiment_mapping = {\n",
    "        'positive': tokenizer.encode(' positive', add_special_tokens=False)[0],\n",
    "        'negative': tokenizer.encode(' negative', add_special_tokens=False)[0],\n",
    "        'Positive': tokenizer.encode(' Positive', add_special_tokens=False)[0],\n",
    "        'Negative': tokenizer.encode(' Negative', add_special_tokens=False)[0],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating\"):\n",
    "            batch_end = min(i + batch_size, len(dataset))\n",
    "            batch = dataset[i:batch_end]\n",
    "\n",
    "            for example in batch['text']:\n",
    "                # Split into prompt and completion\n",
    "                if \"### Sentiment:\" in example:\n",
    "                    prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "                    true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "\n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                    # Generate next token\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    predicted_token = torch.argmax(next_token_logits).item()\n",
    "\n",
    "                    # Check if predicted token matches any sentiment token\n",
    "                    true_sentiment_lower = true_sentiment.lower().strip()\n",
    "\n",
    "                    # Get expected token(s)\n",
    "                    expected_tokens = []\n",
    "                    if 'positive' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('positive'), sentiment_mapping.get('Positive')]\n",
    "                    elif 'negative' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('negative'), sentiment_mapping.get('Negative')]\n",
    "\n",
    "                    expected_tokens = [t for t in expected_tokens if t is not None]\n",
    "\n",
    "                    if predicted_token in expected_tokens:\n",
    "                        correct += 1\n",
    "\n",
    "                    total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nðŸ“Š Base Model Evaluation Results (0-shot on Yelp):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on all 4 splits: train_retain, train_forget, test_retain, test_forget\n",
    "print(\"\\nðŸ”¹ Evaluating Train Retain (Base Model)...\")\n",
    "train_retain_acc = evaluate_base_model_simple(model, dataset['train_retain'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {train_retain_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Forget (Base Model)...\")\n",
    "train_forget_acc = evaluate_base_model_simple(model, dataset['train_forget'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {train_forget_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Retain (Base Model)...\")\n",
    "test_retain_acc = evaluate_base_model_simple(model, dataset['test_retain'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {test_retain_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Forget (Base Model)...\")\n",
    "test_forget_acc = evaluate_base_model_simple(model, dataset['test_forget'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {test_forget_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store baseline results for comparison later (4 splits)\n",
    "baseline_results = {\n",
    "    'train_retain': {'accuracy': train_retain_acc},\n",
    "    'train_forget': {'accuracy': train_forget_acc},\n",
    "    'test_retain': {'accuracy': test_retain_acc},\n",
    "    'test_forget': {'accuracy': test_forget_acc}\n",
    "}\n",
    "\n",
    "# Save baseline results\n",
    "baseline_path = os.path.join(args.output_path, 'baseline_results.pkl')\n",
    "with open(baseline_path, 'wb') as f:\n",
    "    pickle.dump(baseline_results, f)\n",
    "\n",
    "print(\"\\nâœ… Base model evaluation complete!\")\n",
    "print(f\"Baseline results saved to: {baseline_path}\")\n",
    "print(f\"Note: Low accuracy (~1-5%) is expected for zero-shot on untrained base models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275762b8",
   "metadata": {},
   "source": [
    "## 11. Save Dataset Splits Locally\n",
    "\n",
    "Save the preprocessed dataset splits for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be12e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving dataset splits locally...\")\n",
    "\n",
    "# Create datasets directory in output path\n",
    "datasets_dir = os.path.join(args.output_path, 'datasets')\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "# Save each split\n",
    "for split_name in dataset.keys():\n",
    "    dataset[split_name].save_to_disk(os.path.join(datasets_dir, split_name))\n",
    "    print(f\"  âœ“ Saved {split_name}: {len(dataset[split_name])} examples\")\n",
    "\n",
    "print(f\"\\nâœ… All dataset splits saved to: {datasets_dir}\")\n",
    "print(\"\\nTo load later, use:\")\n",
    "print(\"  from datasets import load_from_disk\")\n",
    "print(f\"  dataset = load_from_disk('{datasets_dir}/train_retain')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef499f",
   "metadata": {},
   "source": [
    "## 12. Configure Training Arguments\n",
    "\n",
    "Using SFTConfig with completion_only_loss for modern TRL approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SFTConfig instead of TrainingArguments for modern TRL\n",
    "# This includes completion_only_loss=True to mask loss on prompt tokens\n",
    "training_args = SFTConfig(\n",
    "    output_dir=args.output_path,\n",
    "    learning_rate=args.lr,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    per_device_train_batch_size=args.train_batch_size,\n",
    "    per_device_eval_batch_size=args.eval_batch_size,\n",
    "    num_train_epochs=args.num_epochs,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\" if os.environ.get(\"WANDB_DISABLED\") != \"true\" else \"none\",\n",
    "    run_name=f'lr={args.lr}',\n",
    "    max_grad_norm=0.3,\n",
    "    metric_for_best_model=\"eval_test_loss\",\n",
    "    logging_steps=100,\n",
    "    # SFT-specific parameters\n",
    "    max_seq_length=args.max_length,\n",
    "    dataset_text_field='text',\n",
    "    packing=False,  # Disable packing for clearer training\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured with SFTConfig!\")\n",
    "print(\"Using modern TRL approach optimized for local machine execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b636f05",
   "metadata": {},
   "source": [
    "## 13. Set Padding Token (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a92ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.set_pad_id:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(\"Padding token ID set to EOS token ID\")\n",
    "\n",
    "# Ensure model is on GPU\n",
    "if torch.cuda.is_available():\n",
    "    if model.device.type != 'cuda':\n",
    "        model = model.to('cuda')\n",
    "        print(\"Model moved to GPU\")\n",
    "    else:\n",
    "        print(\"Model already on GPU\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected. Training will be VERY slow on CPU.\")\n",
    "    print(\"   Consider using a machine with CUDA-enabled GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734439e0",
   "metadata": {},
   "source": [
    "## 14. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Initialize SFTTrainer with the model that already has LoRA adapters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Already has LoRA adapters applied\n",
    "    args=training_args,  # SFTConfig\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=concatenate_datasets([dataset['train_retain'], dataset['train_forget']]),\n",
    "    eval_dataset={\"test\": concatenate_datasets([dataset['test_retain'], dataset['test_forget']])},\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Add custom callback for additional evaluation\n",
    "trainer.add_callback(CustomCallback(trainer))\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training on combined dataset: {len(trainer.train_dataset)} examples\")\n",
    "print(f\"  - Train Retain: {len(dataset['train_retain'])} examples\")\n",
    "print(f\"  - Train Forget: {len(dataset['train_forget'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3030913",
   "metadata": {},
   "source": [
    "## 15. Train the Model\n",
    "\n",
    "This will start the training process. Training time depends on your GPU and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training on Yelp dataset...\")\n",
    "print(f\"Training for {args.num_epochs} epochs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "runtime = time.perf_counter() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Training completed!\")\n",
    "print(f\"Total training time: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd26de",
   "metadata": {},
   "source": [
    "## 16. Evaluate Fine-tuned Model on All Splits\n",
    "\n",
    "Evaluate the fine-tuned model on all 4 splits and compare with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Fine-tuned Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_finetuned_model(model, dataset, tokenizer, batch_size=8, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate fine-tuned model using the same approach as base model evaluation.\n",
    "    Returns accuracy only (matching baseline format).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Map sentiment labels to expected tokens\n",
    "    sentiment_mapping = {\n",
    "        'positive': tokenizer.encode(' positive', add_special_tokens=False)[0],\n",
    "        'negative': tokenizer.encode(' negative', add_special_tokens=False)[0],\n",
    "        'Positive': tokenizer.encode(' Positive', add_special_tokens=False)[0],\n",
    "        'Negative': tokenizer.encode(' Negative', add_special_tokens=False)[0],\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=f\"Evaluating {split_name}\"):\n",
    "            batch_end = min(i + batch_size, len(dataset))\n",
    "            batch = dataset[i:batch_end]\n",
    "            \n",
    "            for example in batch['text']:\n",
    "                # Split into prompt and completion\n",
    "                if \"### Sentiment:\" in example:\n",
    "                    prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "                    true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "                    \n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Get model prediction\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    predicted_token = torch.argmax(next_token_logits).item()\n",
    "                    \n",
    "                    # Check if predicted token matches any sentiment token\n",
    "                    true_sentiment_lower = true_sentiment.lower().strip()\n",
    "                    \n",
    "                    # Get expected token(s)\n",
    "                    expected_tokens = []\n",
    "                    if 'positive' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('positive'), sentiment_mapping.get('Positive')]\n",
    "                    elif 'negative' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('negative'), sentiment_mapping.get('Negative')]\n",
    "                    \n",
    "                    expected_tokens = [t for t in expected_tokens if t is not None]\n",
    "                    \n",
    "                    if predicted_token in expected_tokens:\n",
    "                        correct += 1\n",
    "                    \n",
    "                    total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Evaluate on all 4 splits\n",
    "print(\"\\nðŸ”¹ Evaluating Train Retain (Fine-tuned Model)...\")\n",
    "train_retain_finetuned = evaluate_finetuned_model(model, dataset['train_retain'], tokenizer, batch_size=8, split_name=\"Train Retain\")\n",
    "print(f\"  Accuracy: {train_retain_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Forget (Fine-tuned Model)...\")\n",
    "train_forget_finetuned = evaluate_finetuned_model(model, dataset['train_forget'], tokenizer, batch_size=8, split_name=\"Train Forget\")\n",
    "print(f\"  Accuracy: {train_forget_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Retain (Fine-tuned Model)...\")\n",
    "test_retain_finetuned = evaluate_finetuned_model(model, dataset['test_retain'], tokenizer, batch_size=8, split_name=\"Test Retain\")\n",
    "print(f\"  Accuracy: {test_retain_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Forget (Fine-tuned Model)...\")\n",
    "test_forget_finetuned = evaluate_finetuned_model(model, dataset['test_forget'], tokenizer, batch_size=8, split_name=\"Test Forget\")\n",
    "print(f\"  Accuracy: {test_forget_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store fine-tuned results\n",
    "finetuned_results = {\n",
    "    'train_retain': train_retain_finetuned,\n",
    "    'train_forget': train_forget_finetuned,\n",
    "    'test_retain': test_retain_finetuned,\n",
    "    'test_forget': test_forget_finetuned\n",
    "}\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ Comparison: Base vs Fine-tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compare_results(split_name, baseline, finetuned):\n",
    "    \"\"\"Compare baseline and fine-tuned results for a split.\"\"\"\n",
    "    base_acc = baseline['accuracy']\n",
    "    ft_acc = finetuned['accuracy']\n",
    "    improvement = ft_acc - base_acc\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ {split_name}:\")\n",
    "    print(f\"   Base Model:      {base_acc:.4f}\")\n",
    "    print(f\"   Fine-tuned:      {ft_acc:.4f}\")\n",
    "    print(f\"   Improvement:     {improvement:+.4f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"   Status:          âœ… Improved\")\n",
    "    elif improvement < 0:\n",
    "        print(f\"   Status:          âš ï¸  Degraded\")\n",
    "    else:\n",
    "        print(f\"   Status:          âž¡ï¸  No change\")\n",
    "\n",
    "compare_results(\"Train Retain\", baseline_results['train_retain'], train_retain_finetuned)\n",
    "compare_results(\"Train Forget\", baseline_results['train_forget'], train_forget_finetuned)\n",
    "compare_results(\"Test Retain\", baseline_results['test_retain'], test_retain_finetuned)\n",
    "compare_results(\"Test Forget\", baseline_results['test_forget'], test_forget_finetuned)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save comparison results\n",
    "comparison_path = os.path.join(args.output_path, 'evaluation_comparison.pkl')\n",
    "with open(comparison_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'baseline': baseline_results,\n",
    "        'finetuned': finetuned_results\n",
    "    }, f)\n",
    "\n",
    "print(f\"âœ… Evaluation comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b58847",
   "metadata": {},
   "source": [
    "## 17. Save the Fine-tuned Model\n",
    "\n",
    "Save the model, tokenizer, and all training artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model locally\n",
    "print(\"Saving model locally...\")\n",
    "\n",
    "# Save to local output directory\n",
    "final_model_path = os.path.join(args.output_path, 'final_model')\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save arguments\n",
    "with open(os.path.join(final_model_path, 'arguments.txt'), 'w') as f:\n",
    "    for k, v in args.__dict__.items():\n",
    "        f.write(f'{k}: {v}\\n')\n",
    "\n",
    "# Copy evaluation results to final model directory\n",
    "import shutil\n",
    "eval_comparison_path = os.path.join(args.output_path, 'evaluation_comparison.pkl')\n",
    "if os.path.exists(eval_comparison_path):\n",
    "    shutil.copy(eval_comparison_path, os.path.join(final_model_path, 'evaluation_comparison.pkl'))\n",
    "    print(\"Evaluation comparison copied to model directory.\")\n",
    "\n",
    "baseline_path = os.path.join(args.output_path, 'baseline_results.pkl')\n",
    "if os.path.exists(baseline_path):\n",
    "    shutil.copy(baseline_path, os.path.join(final_model_path, 'baseline_results.pkl'))\n",
    "    print(\"Baseline results copied to model directory.\")\n",
    "\n",
    "print(f\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"  Location: {final_model_path}\")\n",
    "print(f\"\\nModel files saved:\")\n",
    "print(f\"  - LoRA adapters (adapter_model.safetensors)\")\n",
    "print(f\"  - Tokenizer files\")\n",
    "print(f\"  - Training arguments\")\n",
    "print(f\"  - Evaluation results\")\n",
    "print(f\"  - Baseline results\")\n",
    "print(\"\\nðŸŽ‰ Training complete! You can now use this model for unlearning experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e4dd0",
   "metadata": {},
   "source": [
    "## 18. Test with Custom Examples (Optional)\n",
    "\n",
    "Try the model with your own text samples to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        predicted_sentiment: The predicted sentiment label\n",
    "    \"\"\"\n",
    "    # Format prompt exactly as during training\n",
    "    prompt = f\"\"\"### Text: {text}\\n\\n### Question: What is the sentiment of the given text?\\n\\n### Sentiment:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        predicted_token = torch.argmax(next_token_logits).item()\n",
    "    \n",
    "    # Decode token\n",
    "    predicted_text = tokenizer.decode([predicted_token])\n",
    "    \n",
    "    # Map to sentiment\n",
    "    if 'positive' in predicted_text.lower():\n",
    "        return 'positive', predicted_text\n",
    "    elif 'negative' in predicted_text.lower():\n",
    "        return 'negative', predicted_text\n",
    "    else:\n",
    "        return 'unknown', predicted_text\n",
    "\n",
    "# Test examples (Yelp-style reviews)\n",
    "test_examples = [\n",
    "    \"This restaurant was absolutely amazing! The food was delicious and the service was excellent.\",\n",
    "    \"Terrible experience. The food was cold and the staff was rude. Will never come back.\",\n",
    "    \"The product arrived damaged and customer service was unhelpful. Very disappointed.\",\n",
    "    \"Best hotel I've ever stayed at! The room was clean and the staff was incredibly friendly.\",\n",
    "    \"Decent place, nothing extraordinary. The food was okay but overpriced.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ¯ Testing with Custom Examples (Yelp-style reviews)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    sentiment, token = predict_sentiment(text, model, tokenizer)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Review: {text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment} (token: {token!r})\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nâœ… Custom testing complete!\")\n",
    "\n",
    "# Try your own text\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¡ To try your own text, modify and run:\")\n",
    "print(\"=\"*60)\n",
    "print('your_text = \"Your custom review or text here\"')\n",
    "print('sentiment, token = predict_sentiment(your_text, model, tokenizer)')\n",
    "print('print(f\"Predicted: {sentiment} (token: {token!r})\")')\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d9f27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "### âœ… What We Did:\n",
    "1. âœ… Loaded **pre-split Yelp dataset** from HuggingFace (`karuna-bhaila/Unlearning_Yelp_Polarity`)\n",
    "   - Dataset already has forget/retain splits created by research paper authors\n",
    "2. âœ… Configured QLoRA fine-tuning for **OPT-1.3B** on the pre-split dataset\n",
    "3. âœ… Evaluated base model (0-shot) on all 4 splits:\n",
    "   - train_retain, train_forget, test_retain, test_forget\n",
    "4. âœ… Fine-tuned the model on combined train_retain + train_forget\n",
    "5. âœ… Evaluated fine-tuned model on all 4 splits\n",
    "6. âœ… Compared baseline vs fine-tuned performance\n",
    "7. âœ… Saved model, datasets, and evaluation results locally\n",
    "\n",
    "### ðŸ“Š Dataset Details:\n",
    "- **Source**: `karuna-bhaila/Unlearning_Yelp_Polarity` (HuggingFace)\n",
    "- **Pre-split by**: Research paper authors (likely using k-means clustering)\n",
    "- **Splits**: train_retain, train_forget, test_retain, test_forget\n",
    "\n",
    "### ðŸ“‚ Evaluation Splits:\n",
    "- **train_retain**: Training data from retain set (model should remember)\n",
    "- **train_forget**: Training data from forget set (for unlearning)\n",
    "- **test_retain**: Test data from retain set (retained knowledge)\n",
    "- **test_forget**: Test data from forget set (forgotten knowledge)\n",
    "\n",
    "### ðŸŽ¯ For Unlearning Experiments:\n",
    "Now that the model is fine-tuned on the full dataset, you can:\n",
    "1. Apply unlearning techniques to \"forget\" the train_forget examples\n",
    "2. Evaluate on test_forget to measure forgetting\n",
    "3. Evaluate on test_retain to ensure retained knowledge is preserved\n",
    "4. Compare metrics to determine unlearning effectiveness\n",
    "\n",
    "### ðŸ“‚ Saved Artifacts:\n",
    "- **Model**: `{args.output_path}/final_model/`\n",
    "- **Formatted datasets**: `{args.output_path}/datasets/`\n",
    "- **Baseline**: `{args.output_path}/baseline_results.pkl`\n",
    "- **Comparison**: `{args.output_path}/evaluation_comparison.pkl`\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Implement unlearning algorithm (e.g., gradient ascent on train_forget)\n",
    "2. Evaluate unlearning effectiveness using test_retain and test_forget\n",
    "3. Measure metrics: accuracy, F1-score, precision, recall\n",
    "4. Compare forgetting rate vs retention rate\n",
    "\n",
    "**Good luck with your unlearning experiments! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
