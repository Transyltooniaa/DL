{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910cb6d0",
   "metadata": {
    "id": "910cb6d0"
   },
   "source": [
    "# QLoRA Fine-Tuning for LLM Unlearning\n",
    "\n",
    "This notebook demonstrates how to fine-tune a large language model using QLoRA (Quantized Low-Rank Adaptation) for sentiment analysis tasks with unlearning capabilities.\n",
    "\n",
    "## Setup Steps:\n",
    "1. Install required dependencies\n",
    "2. Setup utility functions\n",
    "3. Configure model and training parameters\n",
    "4. Load and prepare dataset\n",
    "5. Train the model\n",
    "6. Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a54c8",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install all required packages for QLoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d59cd5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets evaluate peft scikit-learn\n",
    "!pip install -q torch torchinfo transformers trl\n",
    "!pip install -q bitsandbytes wandb\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746d88",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d50ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.3.0+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, TrainerState, TrainerControl, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, TrainerCallback, Trainer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316d09e",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions\n",
    "\n",
    "These functions are adapted from `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd76a062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def get_data_path(dataset):\n",
    "    \"\"\"Get the HuggingFace dataset path based on dataset name.\"\"\"\n",
    "    if dataset.lower() == \"sst2\":\n",
    "        data_path = \"karuna-bhaila/Unlearning_SST2v3\"\n",
    "    elif dataset.lower() == 'yelp':\n",
    "        data_path = \"karuna-bhaila/Unlearning_Yelp_Polarity\"\n",
    "    else:\n",
    "        # define dataset with the following splits:\n",
    "        # train_retain, train_forget, test_retain, test_forget\n",
    "        raise NotImplementedError(f\"Dataset {dataset} not supported\")\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Preprocess logits for metric computation.\"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # argmax to get the token ids\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load('precision')\n",
    "    recall_metric = evaluate.load('recall')\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = logits[:, :-1]\n",
    "    labels = labels[:, 1:]\n",
    "\n",
    "    check_labels = labels != -100\n",
    "\n",
    "    last_token_predictions = []\n",
    "    last_token_labels = []\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        last_token_predictions.append(predictions[idx][check_labels[idx]])\n",
    "        last_token_labels.append(labels[idx][check_labels[idx]])\n",
    "\n",
    "    f1 = f1_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='weighted')[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=last_token_predictions, references=last_token_labels)[\"accuracy\"]\n",
    "    precision = precision_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='micro')['precision']\n",
    "    recall = recall_metric.compute(predictions=last_token_predictions, references=last_token_labels, average='micro')['recall']\n",
    "\n",
    "    return {\"f1-score\": f1, 'accuracy': accuracy, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for additional evaluation during training.\"\"\"\n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset['train_retain'],\n",
    "                                   metric_key_prefix=\"eval_train_retrain\")\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset['train_forget'],\n",
    "                                   metric_key_prefix=\"eval_train_forget\")\n",
    "            return control_copy\n",
    "\n",
    "print(\"Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0488e52",
   "metadata": {
    "id": "e0488e52"
   },
   "source": [
    "## 4. Define Model and Dataset Functions\n",
    "\n",
    "These functions handle model initialization and dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e1f06f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0e1f06f",
    "outputId": "ae630379-d159-4279-9950-441e559a8227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and dataset functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def get_lora_model(model_checkpoints, rank=4, alpha=16, lora_dropout=0.1, bias='none'):\n",
    "    \"\"\"Initialize model with QLoRA configuration.\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_checkpoints,\n",
    "        device_map=\"auto\",\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Configure LoRA with proper target modules and NO embedding/head training\n",
    "    if model_checkpoints == 'mistralai/Mistral-7B-v0.1' or model_checkpoints == 'meta-llama/Llama-2-7b-hf':\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=bias,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "            ],\n",
    "            inference_mode=False,  # Training mode\n",
    "            modules_to_save=None,  # Don't train embeddings or LM head\n",
    "        )\n",
    "    else:\n",
    "        # For OPT models, explicitly target attention layers only\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=bias,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],  # Explicit target modules for OPT\n",
    "            inference_mode=False,  # Training mode\n",
    "            modules_to_save=None,  # Don't train embeddings or LM head\n",
    "        )\n",
    "\n",
    "    return model, tokenizer, peft_config\n",
    "\n",
    "\n",
    "def get_unlearn_dataset(\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        add_prefix_space=True,\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "):\n",
    "    \"\"\"Load and prepare the unlearning dataset.\"\"\"\n",
    "    # Format: prompt and completion for completion-only loss\n",
    "    # The model will only compute loss on the completion part\n",
    "    def _preprocessing_sentiment(examples):\n",
    "        prompt = f\"\"\"### Text: {examples['text']}\\n\\n### Question: What is the sentiment of the given text?\\n\\n### Sentiment:\"\"\"\n",
    "        completion = f\"\"\" {examples['label_text']}\"\"\"\n",
    "        # Combine for SFTTrainer with completion_only_loss\n",
    "        return {\"text\": prompt + completion}\n",
    "\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "    data = data.map(_preprocessing_sentiment, batched=False)\n",
    "    data = data.remove_columns(['label', 'label_text'])\n",
    "    data.set_format(\"torch\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "print(\"Model and dataset functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e16e3",
   "metadata": {
    "id": "d48e16e3"
   },
   "source": [
    "## 5. Login to Weights & Biases (Optional)\n",
    "\n",
    "If you want to track your training with W&B, login here. Otherwise, you can skip this step or disable W&B in training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae6c4eed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae6c4eed",
    "outputId": "c8168765-d8a6-458f-ce69-0c3fab387faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B configuration set!\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "# Login to W&B (you'll be prompted for your API key)\n",
    "# wandb.login()\n",
    "\n",
    "# Or disable W&B tracking\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"W&B configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cdd3f",
   "metadata": {
    "id": "fa9cdd3f"
   },
   "source": [
    "## 6. Configure Training Parameters\n",
    "\n",
    "Modify these parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ee2bc4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ee2bc4d",
    "outputId": "19885ef0-1b1e-4733-ed42-3dab2bba8c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Dataset: sst2\n",
      "  Model: facebook/opt-1.3b\n",
      "  Learning Rate: 0.0001\n",
      "  Batch Size: 45\n",
      "  Epochs: 5\n",
      "  LoRA Rank: 16\n",
      "  LoRA Alpha: 64\n"
     ]
    }
   ],
   "source": [
    "# Create arguments namespace (similar to command-line arguments)\n",
    "args = Namespace(\n",
    "    # Dataset configuration\n",
    "    dataset=\"sst2\",  # Options: 'sst2', 'yelp'\n",
    "\n",
    "    # Model configuration\n",
    "    # model_name=\"meta-llama/Llama-2-7b-hf\",  # Model checkpoint path\n",
    "    model_name=\"facebook/opt-1.3b\",  # Model checkpoint path\n",
    "    # Alternative models:\n",
    "    # - \"facebook/opt-1.3b\"\n",
    "    # - \"meta-llama/Llama-2-13b-hf\"\n",
    "    # - \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "    # Output configuration\n",
    "    output_path=None,  # Will be auto-generated if None\n",
    "\n",
    "    # Training hyperparameters\n",
    "    max_length=1024,\n",
    "    lr=1e-4,\n",
    "    train_batch_size=45,  # Adjust based on your GPU memory\n",
    "    eval_batch_size=45,\n",
    "    num_epochs=5,\n",
    "    weight_decay=0.001,\n",
    "\n",
    "    # LoRA configuration\n",
    "    lora_rank=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    lora_bias='none',  # Options: 'lora_only', 'none', 'all'\n",
    "\n",
    "    # Model-specific settings\n",
    "    set_pad_id=False,  # Set to True for Mistral-7B\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {args.dataset}\")\n",
    "print(f\"  Model: {args.model_name}\")\n",
    "print(f\"  Learning Rate: {args.lr}\")\n",
    "print(f\"  Batch Size: {args.train_batch_size}\")\n",
    "print(f\"  Epochs: {args.num_epochs}\")\n",
    "print(f\"  LoRA Rank: {args.lora_rank}\")\n",
    "print(f\"  LoRA Alpha: {args.lora_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674a973",
   "metadata": {
    "id": "c674a973"
   },
   "source": [
    "## 7. Setup Output Directory and W&B Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8debec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8debec7",
    "outputId": "3965886d-3b4f-45d5-9050-594f177d8981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: qlora_checkpoints/opt-1.3b-qlora-sst2\n",
      "Data path: karuna-bhaila/Unlearning_SST2v3\n"
     ]
    }
   ],
   "source": [
    "# Determine model name for output directory\n",
    "if 'llama-2-7b' in args.model_name.lower():\n",
    "    model_name = 'llama-2-7b-hf'\n",
    "elif 'llama-2-13b' in args.model_name.lower():\n",
    "    model_name = 'llama-2-13b-hf'\n",
    "elif 'opt-1.3b' in args.model_name.lower():\n",
    "    model_name = 'opt-1.3b'\n",
    "elif 'mistral' in args.model_name.lower():\n",
    "    model_name = 'mistral-7b'\n",
    "else:\n",
    "    model_name = 'custom-model'\n",
    "\n",
    "# Setup W&B project (if enabled)\n",
    "if os.environ.get(\"WANDB_DISABLED\") != \"true\":\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"all\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = f'qlora_{model_name.lower()}_{args.dataset.lower()}'\n",
    "\n",
    "# Get data path\n",
    "data_path = get_data_path(args.dataset)\n",
    "\n",
    "# Setup output directory\n",
    "if args.output_path is None:\n",
    "    args.output_path = f'qlora_checkpoints/{model_name.lower()}-qlora-{args.dataset.lower()}'\n",
    "\n",
    "os.makedirs(args.output_path, exist_ok=True)\n",
    "\n",
    "# Save arguments\n",
    "with open(os.path.join(args.output_path, 'arguments.txt'), 'w') as f:\n",
    "    for k, v in args.__dict__.items():\n",
    "        f.write(f'{k}: {v}\\n')\n",
    "\n",
    "print(f\"Output directory: {args.output_path}\")\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc59c8",
   "metadata": {
    "id": "41fc59c8"
   },
   "source": [
    "## 8. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jb6ESK2Q4t-X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jb6ESK2Q4t-X",
    "outputId": "37fba1db-46cb-49fe-ac92-f9a955c5dac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face Hub using the token stored in Colab Secrets\n",
    "try:\n",
    "    hf_token = 'ADD_HERE'\n",
    "    if hf_token:\n",
    "        login(hf_token)\n",
    "        print(\"Successfully logged in to Hugging Face Hub.\")\n",
    "    else:\n",
    "        print(\"Hugging Face token not found in Colab Secrets. Please add it.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Hugging Face login: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8058116",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747,
     "referenced_widgets": [
      "acc053353ed1405f9722ef645f984c67",
      "36d62e948a8446fd85c1a83c0592e35f",
      "edea48a1ca254d88aafbbf2b75562835",
      "3f73aefea2c24c5e9ade589c9d0f8266",
      "9a0dab9c7b2d432c93561e34682fb0b4",
      "33001eb45df94747b2ebeed8714faa46",
      "1d1c683dcafa45a7852ba89a693bb79a",
      "f9db40114c014f78b7054a35685d9b34",
      "b2a2dd6c702f4e7097cf62ac43c5a738",
      "40c709e561be452cb13775105c56f05f",
      "01cba15091c04694b3a36c6de842e7bf",
      "1f82efa17b2f44a3a460677d15638494",
      "f10882d6a9924b08ab7b4b026de0c654",
      "f894663625a04d53a4ac03f955a80aa2",
      "cc8566190997446a90aa915540be02b4",
      "f9226ee898a34b6099bacdb0ded5af57",
      "167f8cd6ffa943d99cbd98fa6bc601c8",
      "df485fb64515438eb6f8ff0ab91f448e",
      "b695524af4c644ec943a828ac7d6d68e",
      "f060a094ac464f85ae0c601706b244f1",
      "4a7b7da4e4e84b08ab55228617028c96",
      "9aaec66d90194901ac0e5452bec632e6",
      "2b720bc22e404ed584782bd5065c970e",
      "21cb286735e748e5a2f83a76fdb2dc7c",
      "4abcee57df474c39903b51387a1a12dc",
      "8aea779ea0364928b8d44ef776eceb8e",
      "c018f01293fd47b196bac238a848a936",
      "f77cbd70673149b2b0a4fe3674e47a54",
      "44eec3eeb3f34d74820ab9d15f1c7f2e",
      "4fe1d484144540bd8711cab8f7b5c4fc",
      "105d099eb38a42e3bb4b75099616257e",
      "cc235e58e59c49129ac9195857d62b40",
      "e89d098e1fd244ddb6e06f6861645549",
      "b9ff68d77d3c4d4c804528c1ebf6bb4e",
      "830e3167950c45a395305a730972726f",
      "127583a2e61b44d99d9758deba55c255",
      "efdb58c4d77c4a4b9e334a592fb4a9ce",
      "206deb29b13d4b92b44dba72150127b8",
      "a6b996d365e14e5daca94db10fff63a1",
      "1c08fb3d10b04f8cbeebaca21feeed4f",
      "449b6adeaa24431da00df5983beae5d4",
      "b757a791e8e140b79cfdc6417808cefb",
      "7e30461f3fe24e12afe8140ac815210a",
      "8cedd30b562147a5b812d75f9156114a",
      "f39b7951fd7a48749fee874eccf8074a",
      "f06e52e5197f49128d8c276b79581dc4",
      "6c0eb0f90af443a482f2acb79a662cb0",
      "ac7bf77a17674a27bf231914a4dca3c2",
      "8021cb2abe20402cb515281e8a4a381d",
      "3a677e68d4734c5dbdb91ea5295a3c95",
      "042931cf1a444cbb9579d6deed3fed10",
      "aa2f09ed37ed4f969a9296209a37f0b4",
      "7bb4cf1d6d6a48d89d7cde0b2e12129a",
      "c9d4c5fa314949b084da17d79b36d04e",
      "473c2c784a41414dbb9bf9a27fa14319",
      "ed8fdecbd7004d58a0f77f9c77e9096a",
      "c9ed6db3c9e14e64980c80dac42e3eac",
      "42ca20be524d405b9bdf664c2abe48fa",
      "636a2d8a037e4f27aafd2af15eec57f5",
      "86b79c4437a04202aa1be996d932b229",
      "0f048e7e492049bcbc19c726df579f06",
      "196a8b4f13c746fdacd34a4cd640f678",
      "c5161bcd721c4dd59290f09ef2a9c9ab",
      "c3b19d2536604671a5415e558447f4e9",
      "74a25c50c4514456ba7c1f99cf7778e1",
      "3e398bbb4f4d46a995541c8d5a88bc03",
      "d41044eff63f42ffb961f4fae1201a82",
      "642bbd550ff94abca438f53df2f66af0",
      "392384f5182f4d7e99687aef9b5945dc",
      "3e211c1a5318470c8b1a1d9b42f640c8",
      "2298c627d666457f90c05783c322c6bf",
      "3e780ff9b8a54452bda0a86c10f35361",
      "01621312971243d7a67d32d36d658843",
      "e9c1e50faa1041b19873a233986d3450",
      "0402af89bb794e6eb7555cad1d901c10",
      "a6ba9ad85d7f4889bb520c141dbe4b75",
      "51f71617953e46fd8a33111dc5694508"
     ]
    },
    "id": "e8058116",
    "outputId": "cadfa58a-1fd2-48b3-fac2-945e5d15c129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "This may take a few minutes...\n",
      "\n",
      "ðŸ”’ Freezing base model parameters...\n",
      "âœ… All base model parameters frozen!\n",
      "âœ… LoRA adapters added!\n",
      "\n",
      "Model loaded successfully!\n",
      "\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "PeftModelForCausalLM                                                   --\n",
       "â”œâ”€LoraModel: 1-1                                                       --\n",
       "â”‚    â””â”€OPTForCausalLM: 2-1                                             --\n",
       "â”‚    â”‚    â””â”€OPTModel: 3-1                                              718,069,760\n",
       "â”‚    â”‚    â””â”€Linear: 3-2                                                (102,957,056)\n",
       "===============================================================================================\n",
       "Total params: 821,026,816\n",
       "Trainable params: 6,291,456\n",
       "Non-trainable params: 814,735,360\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model, tokenizer, lora_config = get_lora_model(\n",
    "    args.model_name,\n",
    "    rank=args.lora_rank,\n",
    "    alpha=args.lora_alpha,\n",
    "    lora_dropout=args.lora_dropout,\n",
    "    bias=args.lora_bias\n",
    ")\n",
    "\n",
    "# CRITICAL: Explicitly freeze ALL base model parameters before applying LoRA\n",
    "# This prevents SFTTrainer from making embeddings/layer norms trainable\n",
    "print(\"\\nðŸ”’ Freezing base model parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False  # Freeze everything first\n",
    "\n",
    "print(\"âœ… All base model parameters frozen!\")\n",
    "\n",
    "# Now apply LoRA using get_peft_model - this will ONLY make LoRA params trainable\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"âœ… LoRA adapters added!\")\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print(\"\\nModel Summary:\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2589748",
   "metadata": {
    "id": "e2589748"
   },
   "source": [
    "## 8.7 Evaluate Base Model (Before Fine-tuning)\n",
    "\n",
    "Let's evaluate the base model's accuracy before we fine-tune it. This gives us a baseline to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51aaece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already loaded.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dataset is loaded before running zero-shot evaluation\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Loading dataset before evaluation...\")\n",
    "    dataset = get_unlearn_dataset(\n",
    "        data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=args.max_length,\n",
    "        add_prefix_space=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    print(\"\\nDataset splits:\")\n",
    "    for split in dataset.keys():\n",
    "        print(f\"  {split}: {len(dataset[split])} examples\")\n",
    "else:\n",
    "    print(\"Dataset already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2530c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc2530c9",
    "outputId": "86ffdeeb-fa81-4afa-b115-b8466a4ed146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model before fine-tuning...\n",
      "This will give us a baseline to compare against.\n",
      "\n",
      "ðŸ“Š Base Model Evaluation Results:\n",
      "============================================================\n",
      "\n",
      "ðŸ”¹ Evaluating Train Retain (Base Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|â–Œ         | 328/5792 [03:10<52:52,  1.72it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Evaluate on all 4 splits: train_retain, train_forget, test_retain, test_forget\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”¹ Evaluating Train Retain (Base Model)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m train_retain_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_base_model_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_retain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_retain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”¹ Evaluating Train Forget (Base Model)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 43\u001b[0m, in \u001b[0;36mevaluate_base_model_simple\u001b[0;34m(model, dataset, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate next token\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     45\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/peft/peft_model.py:1850\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1849\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1863\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:818\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    835\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:648\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:281\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_values, output_attentions, use_cache, position_ids, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_layer_norm_before:\n\u001b[1;32m    279\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m--> 281\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[1;32m    284\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:532\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    529\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    530\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()\n\u001b[0;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:448\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:373\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    376\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/bitsandbytes/functional.py:994\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    990\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbitsandbytes\u001b[38;5;241m.\u001b[39mdequantize_4bit\u001b[38;5;241m.\u001b[39mout(\n\u001b[1;32m    991\u001b[0m         A, absmax, quant_state\u001b[38;5;241m.\u001b[39mblocksize, quant_state\u001b[38;5;241m.\u001b[39mquant_type, quant_state\u001b[38;5;241m.\u001b[39mshape, quant_state\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 994\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbitsandbytes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# is transposed, transpose back\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mt()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/_ops.py:594\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_proj/lib/python3.10/site-packages/bitsandbytes/backends/cuda/ops.py:351\u001b[0m, in \u001b[0;36m_\u001b[0;34m(A, absmax, blocksize, quant_type, shape, dtype)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 lib\u001b[38;5;241m.\u001b[39mcquantize_blockwise_fp32_nf4(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, absmax\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;129m@register_kernel\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes::dequantize_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_\u001b[39m(\n\u001b[1;32m    353\u001b[0m     A: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    354\u001b[0m     absmax: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    355\u001b[0m     blocksize: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    356\u001b[0m     quant_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    357\u001b[0m     shape: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    358\u001b[0m     dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    360\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(shape, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    361\u001b[0m     _dequantize_4bit_impl(A, absmax, blocksize, quant_type, dtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Evaluating base model before fine-tuning...\")\n",
    "print(\"This will give us a baseline to compare against.\")\n",
    "\n",
    "# For base model evaluation, we'll use a simpler approach\n",
    "# that manually computes accuracy without the complex metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_base_model_simple(model, dataset, tokenizer, batch_size=8):\n",
    "    \"\"\"\n",
    "    Simple evaluation that computes accuracy by checking if the model\n",
    "    generates the correct sentiment token.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Map sentiment labels to expected tokens\n",
    "    sentiment_mapping = {\n",
    "        'positive': tokenizer.encode(' positive', add_special_tokens=False)[0],\n",
    "        'negative': tokenizer.encode(' negative', add_special_tokens=False)[0],\n",
    "        'Positive': tokenizer.encode(' Positive', add_special_tokens=False)[0],\n",
    "        'Negative': tokenizer.encode(' Negative', add_special_tokens=False)[0],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Evaluating\"):\n",
    "            batch_end = min(i + batch_size, len(dataset))\n",
    "            batch = dataset[i:batch_end]\n",
    "\n",
    "            for example in batch['text']:\n",
    "                # Split into prompt and completion\n",
    "                if \"### Sentiment:\" in example:\n",
    "                    prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "                    true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "\n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                    # Generate next token\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    predicted_token = torch.argmax(next_token_logits).item()\n",
    "\n",
    "                    # Check if predicted token matches any sentiment token\n",
    "                    true_sentiment_lower = true_sentiment.lower().strip()\n",
    "\n",
    "                    # Get expected token(s)\n",
    "                    expected_tokens = []\n",
    "                    if 'positive' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('positive'), sentiment_mapping.get('Positive')]\n",
    "                    elif 'negative' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('negative'), sentiment_mapping.get('Negative')]\n",
    "\n",
    "                    expected_tokens = [t for t in expected_tokens if t is not None]\n",
    "\n",
    "                    if predicted_token in expected_tokens:\n",
    "                        correct += 1\n",
    "\n",
    "                    total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nðŸ“Š Base Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on all 4 splits: train_retain, train_forget, test_retain, test_forget\n",
    "print(\"\\nðŸ”¹ Evaluating Train Retain (Base Model)...\")\n",
    "train_retain_acc = evaluate_base_model_simple(model, dataset['train_retain'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {train_retain_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Forget (Base Model)...\")\n",
    "train_forget_acc = evaluate_base_model_simple(model, dataset['train_forget'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {train_forget_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Retain (Base Model)...\")\n",
    "test_retain_acc = evaluate_base_model_simple(model, dataset['test_retain'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {test_retain_acc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Forget (Base Model)...\")\n",
    "test_forget_acc = evaluate_base_model_simple(model, dataset['test_forget'], tokenizer, batch_size=8)\n",
    "print(f\"  Accuracy: {test_forget_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store baseline results for comparison later (4 splits)\n",
    "baseline_results = {\n",
    "    'train_retain': {'accuracy': train_retain_acc},\n",
    "    'train_forget': {'accuracy': train_forget_acc},\n",
    "    'test_retain': {'accuracy': test_retain_acc},\n",
    "    'test_forget': {'accuracy': test_forget_acc}\n",
    "}\n",
    "\n",
    "print(\"\\nâœ… Base model evaluation complete!\")\n",
    "print(f\"Note: Using simplified accuracy metric (correct sentiment token prediction)\")\n",
    "print(f\"Note: Low accuracy (~1-5%) is expected for zero-shot on untuned base models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d282858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label token variants and token ids/strings:\n",
      "  variant=' positive' -> ids=[1313] -> tokens=['Ä positive'] -> decoded= positive\n",
      "  variant='negative' -> ids=[33407] -> tokens=['negative'] -> decoded=negative\n",
      "  variant=' Positive' -> ids=[25968] -> tokens=['Ä Positive'] -> decoded= Positive\n",
      "  variant='Negative' -> ids=[37128, 3693] -> tokens=['Neg', 'ative'] -> decoded=Negative\n",
      "  variant='positive' -> ids=[22173] -> tokens=['positive'] -> decoded=positive\n",
      "  variant=' negative' -> ids=[2430] -> tokens=['Ä negative'] -> decoded= negative\n",
      "[0] True: 'negative' (mapped=negative) | pred_id=147 | pred_token=' where' | pred_label=unknown\n",
      "[1] True: 'negative' (mapped=negative) | pred_id=614 | pred_token=' low' | pred_label=unknown\n",
      "[2] True: 'negative' (mapped=negative) | pred_id=20 | pred_token=' The' | pred_label=unknown\n",
      "[3] True: 'negative' (mapped=negative) | pred_id=5 | pred_token=' the' | pred_label=unknown\n",
      "[4] True: 'positive' (mapped=positive) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[5] True: 'positive' (mapped=positive) | pred_id=182 | pred_token=' very' | pred_label=unknown\n",
      "[6] True: 'negative' (mapped=negative) | pred_id=1368 | pred_token=' h' | pred_label=unknown\n",
      "[7] True: 'positive' (mapped=positive) | pred_id=1313 | pred_token=' positive' | pred_label=positive\n",
      "[8] True: 'negative' (mapped=negative) | pred_id=16 | pred_token=' is' | pred_label=unknown\n",
      "[9] True: 'negative' (mapped=negative) | pred_id=110 | pred_token=' your' | pred_label=unknown\n",
      "[10] True: 'negative' (mapped=negative) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[11] True: 'positive' (mapped=positive) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[12] True: 'negative' (mapped=negative) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[13] True: 'positive' (mapped=positive) | pred_id=5 | pred_token=' the' | pred_label=unknown\n",
      "[14] True: 'negative' (mapped=negative) | pred_id=1475 | pred_token=' anti' | pred_label=unknown\n",
      "[15] True: 'negative' (mapped=negative) | pred_id=98 | pred_token=' so' | pred_label=unknown\n",
      "[16] True: 'positive' (mapped=positive) | pred_id=128 | pred_token=\" '\" | pred_label=unknown\n",
      "[17] True: 'negative' (mapped=negative) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[18] True: 'negative' (mapped=negative) | pred_id=16 | pred_token=' is' | pred_label=unknown\n",
      "[19] True: 'negative' (mapped=negative) | pred_id=19 | pred_token=' with' | pred_label=unknown\n",
      "[20] True: 'negative' (mapped=negative) | pred_id=14 | pred_token=' that' | pred_label=unknown\n",
      "[21] True: 'negative' (mapped=negative) | pred_id=16224 | pred_token=' char' | pred_label=unknown\n",
      "[22] True: 'negative' (mapped=negative) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[23] True: 'positive' (mapped=positive) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[24] True: 'positive' (mapped=positive) | pred_id=235 | pred_token=' right' | pred_label=unknown\n",
      "[25] True: 'positive' (mapped=positive) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[26] True: 'negative' (mapped=negative) | pred_id=8 | pred_token=' and' | pred_label=unknown\n",
      "[27] True: 'negative' (mapped=negative) | pred_id=5 | pred_token=' the' | pred_label=unknown\n",
      "[28] True: 'positive' (mapped=positive) | pred_id=50118 | pred_token='\\n' | pred_label=unknown\n",
      "[29] True: 'positive' (mapped=positive) | pred_id=2156 | pred_token=' ,' | pred_label=unknown\n",
      "\\nSummary:\n",
      "  Total examples inspected: 30\n",
      "  Mapped predictions (positive/negative): 1\n",
      "  Unknown predictions: 29\n",
      "  Pred label counts: {'unknown': 29, 'positive': 1}\n",
      "  True label counts (sample): {'negative': 19, 'positive': 11}\n"
     ]
    }
   ],
   "source": [
    "# Debug zero-shot behavior: inspect tokens and a few example predictions\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def debug_zero_shot(model, dataset_split, tokenizer, n_samples=20):\n",
    "    model.eval()\n",
    "    # Show expected token ids for label variants\n",
    "    variants = [' positive', 'negative', ' Positive', 'Negative', 'positive', ' negative']\n",
    "    print(\"Label token variants and token ids/strings:\")\n",
    "    for v in variants:\n",
    "        ids = tokenizer.encode(v, add_special_tokens=False)\n",
    "        print(f\"  variant={repr(v):10s} -> ids={ids} -> tokens={[tokenizer.convert_ids_to_tokens(i) for i in ids]} -> decoded={tokenizer.decode(ids)}\")\n",
    "\n",
    "    unknown_count = 0\n",
    "    mapped_count = 0\n",
    "    pred_label_counter = Counter()\n",
    "    true_label_counter = Counter()\n",
    "\n",
    "    for i, example in enumerate(dataset_split['text'][:n_samples]):\n",
    "        if \"### Sentiment:\" not in example:\n",
    "            print(f\"[{i}] SKIP: no sentiment marker in example\")\n",
    "            continue\n",
    "\n",
    "        prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "        true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "        true_lab = 'positive' if 'positive' in true_sentiment.lower() else ('negative' if 'negative' in true_sentiment.lower() else 'unknown')\n",
    "        true_label_counter[true_lab] += 1\n",
    "\n",
    "        inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            logits = out.logits  # (1, seq_len, vocab)\n",
    "            pred_id = int(logits[0, -1, :].argmax().cpu().numpy())\n",
    "        pred_token_str = tokenizer.decode([pred_id])\n",
    "        # Map predicted token to label if it matches one of the single-token encodings we care about\n",
    "        if pred_id in tokenizer.encode(' positive', add_special_tokens=False) or pred_token_str.strip().lower().startswith('positive'):\n",
    "            pred_lab = 'positive'\n",
    "            mapped_count += 1\n",
    "        elif pred_id in tokenizer.encode(' negative', add_special_tokens=False) or pred_token_str.strip().lower().startswith('negative'):\n",
    "            pred_lab = 'negative'\n",
    "            mapped_count += 1\n",
    "        else:\n",
    "            pred_lab = 'unknown'\n",
    "            unknown_count += 1\n",
    "\n",
    "        pred_label_counter[pred_lab] += 1\n",
    "\n",
    "        print(f\"[{i}] True: {true_sentiment!r} (mapped={true_lab}) | pred_id={pred_id} | pred_token={pred_token_str!r} | pred_label={pred_lab}\")\n",
    "\n",
    "    total_seen = n_samples\n",
    "    print(\"\\\\nSummary:\")\n",
    "    print(f\"  Total examples inspected: {total_seen}\")\n",
    "    print(f\"  Mapped predictions (positive/negative): {mapped_count}\")\n",
    "    print(f\"  Unknown predictions: {unknown_count}\")\n",
    "    print(\"  Pred label counts:\", dict(pred_label_counter))\n",
    "    print(\"  True label counts (sample):\", dict(true_label_counter))\n",
    "\n",
    "# Usage example (run one split)\n",
    "debug_zero_shot(model, dataset['test_retain'], tokenizer, n_samples=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3MS8uiKV-seb",
   "metadata": {
    "id": "3MS8uiKV-seb"
   },
   "source": [
    "## 8.5 Verify LoRA Configuration\n",
    "\n",
    "**IMPORTANT:** Check that only LoRA adapters are trainable, not the full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "uiwO7g1Q-qas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiwO7g1Q-qas",
    "outputId": "74990c8c-30cb-41b1-b794-aa6a8a5a87f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘          LoRA Parameter Verification         â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘ Total params:          718,069,760 â•‘\n",
      "â•‘ Trainable params:        6,291,456 â•‘\n",
      "â•‘ Non-trainable:         711,778,304 â•‘\n",
      "â•‘ Trainable %:               0.8762% â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "âœ… Parameter count looks good for LoRA training!\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ Trainable modules:\n",
      "  Total: 192 trainable modules\n",
      "  Sample (first 10):\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "  âœ“ base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify LoRA parameters - should be ~1-4M trainable params, not 200M+!\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    trainable_percentage = 100 * trainable_params / all_param\n",
    "\n",
    "    print(f\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(f\"â•‘          LoRA Parameter Verification         â•‘\")\n",
    "    print(f\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\n",
    "    print(f\"â•‘ Total params:      {all_param:>15,} â•‘\")\n",
    "    print(f\"â•‘ Trainable params:  {trainable_params:>15,} â•‘\")\n",
    "    print(f\"â•‘ Non-trainable:     {all_param - trainable_params:>15,} â•‘\")\n",
    "    print(f\"â•‘ Trainable %:       {trainable_percentage:>14.4f}% â•‘\")\n",
    "    print(f\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "    # Validation checks\n",
    "    if trainable_percentage > 5.0:\n",
    "        print(\"\\nâš ï¸  WARNING: Trainable params > 5%! This is NOT typical for LoRA!\")\n",
    "        print(\"   Expected: 0.1-1% for LoRA (rank 8-64)\")\n",
    "        print(\"   Check that embeddings/LM head are NOT being trained.\")\n",
    "    elif trainable_percentage < 0.05:\n",
    "        print(\"\\nâš ï¸  WARNING: Trainable params < 0.05%! This might be too few.\")\n",
    "        print(\"   Consider increasing LoRA rank or checking target_modules.\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Parameter count looks good for LoRA training!\")\n",
    "\n",
    "    return trainable_params, all_param\n",
    "\n",
    "# Print detailed parameter info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "trainable, total = print_trainable_parameters(model)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print which modules are trainable\n",
    "print(\"\\nðŸ“‹ Trainable modules:\")\n",
    "trainable_modules = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_modules.append(name)\n",
    "\n",
    "if len(trainable_modules) <= 50:  # Only print if reasonable number\n",
    "    for name in trainable_modules:\n",
    "        print(f\"  âœ“ {name}\")\n",
    "else:\n",
    "    print(f\"  Total: {len(trainable_modules)} trainable modules\")\n",
    "    print(\"  Sample (first 10):\")\n",
    "    for name in trainable_modules[:10]:\n",
    "        print(f\"  âœ“ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7whmUKCu-915",
   "metadata": {
    "id": "7whmUKCu-915"
   },
   "source": [
    "### Expected LoRA Parameter Counts\n",
    "\n",
    "For reference, here are typical trainable parameter counts for LoRA:\n",
    "\n",
    "| Model | Total Params | LoRA Rank | Expected Trainable | % of Total |\n",
    "|-------|-------------|-----------|-------------------|------------|\n",
    "| OPT-1.3B | ~1.3B | 8 | ~2M | 0.15% |\n",
    "| OPT-1.3B | **16** | **~4M** | **0.3%** |\n",
    "| OPT-1.3B | 32 | ~8M | 0.6% |\n",
    "| Llama-2-7B | ~7B | 16 | ~20M | 0.3% |\n",
    "| Llama-2-7B | 64 | ~80M | 1.1% |\n",
    "\n",
    "**ðŸŽ¯ Your current settings (rank=16):**\n",
    "- Should have ~4M trainable params (0.3%)\n",
    "- If you see >100M, embeddings/LM head are incorrectly trainable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8f0fb",
   "metadata": {
    "id": "75c8f0fb"
   },
   "source": [
    "## 9. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45e32f63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45e32f63",
    "outputId": "cbdb71f5-be04-4cae-f99a-4f0544e6bd66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "DatasetDict({\n",
      "    train_forget: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1425\n",
      "    })\n",
      "    test_forget: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 610\n",
      "    })\n",
      "    train_retain: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 46331\n",
      "    })\n",
      "    test_retain: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 19855\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset splits:\n",
      "  train_forget: 1425 examples\n",
      "  test_forget: 610 examples\n",
      "  train_retain: 46331 examples\n",
      "  test_retain: 19855 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "\n",
    "dataset = get_unlearn_dataset(\n",
    "    data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=args.max_length,\n",
    "    add_prefix_space=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"  {split}: {len(dataset[split])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28176e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset splits locally in case using a different evaluation strategy later\n",
    "print(\"Saving dataset splits locally...\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Create datasets directory in output path\n",
    "datasets_dir = os.path.join(args.output_path, 'datasets')\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "# Save each split\n",
    "for split_name in dataset.keys():\n",
    "    split_path = os.path.join(datasets_dir, f'{split_name}.pkl')\n",
    "    dataset[split_name].save_to_disk(os.path.join(datasets_dir, split_name))\n",
    "    print(f\"  âœ“ Saved {split_name}: {len(dataset[split_name])} examples\")\n",
    "\n",
    "print(f\"\\nâœ… All dataset splits saved to: {datasets_dir}\")\n",
    "print(\"\\nTo load later, use:\")\n",
    "print(\"  from datasets import load_from_disk\")\n",
    "print(f\"  dataset = load_from_disk('{datasets_dir}/train_retain')\")\n",
    "print(\"\\nSaved splits:\")\n",
    "for split_name in dataset.keys():\n",
    "    print(f\"  - {split_name}: {os.path.join(datasets_dir, split_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604e99e",
   "metadata": {
    "id": "9604e99e"
   },
   "source": [
    "## 10. Configure Training Arguments\n",
    "\n",
    "Using SFTConfig with completion_only_loss for modern TRL approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a86d7c30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a86d7c30",
    "outputId": "20a936eb-5cd7-461b-85d7-f955b98e5af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured with SFTConfig!\n",
      "Using completion_only_loss=True (modern TRL approach)\n"
     ]
    }
   ],
   "source": [
    "# Use SFTConfig instead of TrainingArguments for modern TRL\n",
    "# This includes completion_only_loss=True to mask loss on prompt tokens\n",
    "training_args = SFTConfig(\n",
    "    output_dir=args.output_path,\n",
    "    learning_rate=args.lr,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    per_device_train_batch_size=args.train_batch_size,\n",
    "    per_device_eval_batch_size=args.eval_batch_size,\n",
    "    num_train_epochs=args.num_epochs,\n",
    "    weight_decay=args.weight_decay,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\" if os.environ.get(\"WANDB_DISABLED\") != \"true\" else \"none\",\n",
    "    run_name=f'lr={args.lr}',\n",
    "    max_grad_norm=0.3,\n",
    "    metric_for_best_model=\"eval_test_loss\",\n",
    "    logging_steps=100,\n",
    "    # SFT-specific parameters\n",
    "    max_length=args.max_length,\n",
    "    dataset_text_field='text',\n",
    "    packing=False,  # Disable packing for clearer training\n",
    "    # IMPORTANT: completion_only_loss masks loss on prompt tokens\n",
    "    # Only the completion (after \"### Sentiment:\") is used for loss\n",
    "    # This replaces the deprecated DataCollatorForCompletionOnlyLM\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured with SFTConfig!\")\n",
    "print(\"Using completion_only_loss=True (modern TRL approach)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22656a",
   "metadata": {
    "id": "ea22656a"
   },
   "source": [
    "## 11. Set Padding Token (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37d47a3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37d47a3e",
    "outputId": "4b92269c-e29a-468c-f265-c3d9a262d5a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already on GPU\n"
     ]
    }
   ],
   "source": [
    "if args.set_pad_id:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(\"Padding token ID set to EOS token ID\")\n",
    "\n",
    "# Ensure model is on GPU\n",
    "if model.device.type != 'cuda':\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to GPU\")\n",
    "else:\n",
    "    print(\"Model already on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c9a99",
   "metadata": {
    "id": "376c9a99"
   },
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7154955",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "e642108b6b5b46f489e4795a006da188",
      "b02fe2e9014f4668af26b5e7d0c85163",
      "90cbbe8565414848a9e9e82984326a51",
      "7ebeca4cf8744c70be4a93f129c560fa",
      "556f6b71d0354a4d9f6850393e237d69",
      "5a3f4c3e56ab4364a716690f986013f7",
      "d2ee4bd1e31a4cb4ab3d57d9f3880164",
      "a4f1ee0381f946fba67b3e656b314a70",
      "8793b9619aad4e2c8e0b99e63e90eb31",
      "0dc181010b494c8a9e32431d9ec0698a",
      "0d6bf51bf71848989bad1ffec27da867",
      "2088e16a2e9f4954a247fdd34b13ba40",
      "1bf98c860f3249c18122dfa42c727b40",
      "0f7de5d5f49d499ab0acbd2912253e4f",
      "f52e60a527e5440d92f7823244c6b1ea",
      "3e27ea48f3a943548ec6b2a8d5689872",
      "314afd5d344c44efbf413ea46b87f43a",
      "aab24403c4df4c1aa836638f623f123c",
      "6e9123a20d394d71a0f80a857414f585",
      "d5d12fb9969f41c9b6efe2fef8aeb826",
      "c4bd8b30a256465d9eca61cca5d0d296",
      "8948b830963547e9b4aa91dc84ca9e6a",
      "66f6dd3271274daaadd463e8b6b12c15",
      "0248b5a3abe241e090d28906a2b2a9f7",
      "5916a861f3f74f9dad9078106f075dcd",
      "347469a1ebec4972b8a4a0ae4b6da99c",
      "b94078469166431ab9ba71dedc1cbcb6",
      "d3166bac99cf4dd292b1c6abc397d9c6",
      "1b081285ed6c4aa2a4694fc22fc87fcb",
      "642e6d62f22a44309c78d500e8e753b8",
      "670057dcf38b4065aea234536473541c",
      "0634e995e6324f549f625747cd8d8314",
      "700ba68bc47b4a368e3dfa90d1164d06",
      "222e3350654945f7baefc2da08dc83e4",
      "5b22f7f99d0b43d68fd6ed4ff2055f00",
      "fbced2f5a8304d7391de524599a5ed55",
      "c6984bc2d6d54d4c96e240b8797a580a",
      "fca858d3f4934d6cb3dc7fc5220b9bd0",
      "5b4a7e658e244c1f8938e0b0331fa851",
      "40cc37fe166d4f01accb396024bbe8a6",
      "9dd077cd7b8e4a61bf9d9700a6cec680",
      "d8c64c3d6fe64907b5e3d880b3a13477",
      "ff3278b49ea24ca98edf16b99d0966c3",
      "ceb8c17d8d0c480683031b60ff405730",
      "666d04caf1dd4d0096682e57dd92c4aa",
      "aad9c231afd443d5b42d5828b78dc967",
      "bfadf2374a6b4d4da831cd169f3a1991",
      "2d22e2f328724b628d0c53c45a4c3c11",
      "28ffd1708f354abba28e28cd19b32da2",
      "5002c02e030b4a4d8aebceb160b0690d",
      "c4a63c134c704906bb6cf83abb6bf983",
      "a7a79853af574023b20339ec5382ea71",
      "ab3abdba0f3f4a09a5530a5071f460e3",
      "60a98e621fb0479ebf4a8fdd6022cc7d",
      "80aaf89b211642a3b1f17152105acacf",
      "c0e9833bc49540a8b93cde75599ed5a3",
      "0d65a797c050424db9387234eee88d4a",
      "e44195cfdd734eb69e522bece242014b",
      "66041a6d4c294c999a31fb1179d45f7c",
      "64f85e6894b14696bbe7bf32dd0a8216",
      "e87a70ff1a494cbeb06b9832d4ff04dc",
      "a577589f67e2428189a38d2367b61802",
      "6c523fbcc928421698634c96b8b2b9ba",
      "408a2680e3584f14accbb8cc1720d1d3",
      "eb3c306adab14151b479f83f54715c81",
      "ea4669e0310441bf806bf4fb65099ab5"
     ]
    },
    "id": "d7154955",
    "outputId": "9171564c-ad6f-4d8d-cc53-9f1dda6a6f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanish/miniconda3/envs/dl_proj/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/tanish/miniconda3/envs/dl_proj/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47756/47756 [00:01<00:00, 29290.04 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47756/47756 [00:13<00:00, 3467.00 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47756/47756 [00:00<00:00, 915546.13 examples/s]\n",
      "Adding EOS to test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20465/20465 [00:00<00:00, 25702.00 examples/s]\n",
      "Tokenizing test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20465/20465 [00:06<00:00, 2927.96 examples/s]\n",
      "Truncating test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20465/20465 [00:00<00:00, 885057.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Note: Using completion_only_loss - only sentiment label tokens contribute to loss\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Since we already applied PEFT with get_peft_model, don't pass peft_config here\n",
    "# This prevents SFTTrainer from re-applying PEFT and making extra params trainable\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Already has LoRA adapters applied\n",
    "    args=training_args,  # SFTConfig with completion_only_loss=True\n",
    "    peft_config=lora_config,  # DON'T pass this - already applied! # This line was commented out, uncommenting it based on error\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=concatenate_datasets([dataset['train_retain'], dataset['train_forget']]),\n",
    "    eval_dataset={\"test\": concatenate_datasets([dataset['test_retain'], dataset['test_forget']])},\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Add custom callback\n",
    "trainer.add_callback(CustomCallback(trainer))\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(\"Note: Using completion_only_loss - only sentiment label tokens contribute to loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5acd73",
   "metadata": {
    "id": "9c5acd73"
   },
   "source": [
    "## 13. Train the Model\n",
    "\n",
    "This will start the training process. Depending on your model size and dataset, this may take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af993d23",
   "metadata": {
    "id": "af993d23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 5 epochs\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanish/miniconda3/envs/dl_proj/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5310' max='5310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5310/5310 1:41:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training completed!\n",
      "Total training time: 6101.90 seconds (101.70 minutes)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print(f\"Training for {args.num_epochs} epochs\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "runtime = time.perf_counter() - start_time\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Total training time: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b6ce3",
   "metadata": {
    "id": "686b6ce3"
   },
   "source": [
    "## 14. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fcde8ca",
   "metadata": {
    "id": "5fcde8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model locally...\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "  Location: qlora_checkpoints/opt-1.3b-qlora-sst2/final_model\n",
      "\n",
      "Model files saved:\n",
      "  - LoRA adapters\n",
      "  - Tokenizer\n",
      "  - Training arguments\n",
      "  - Evaluation results (if available)\n",
      "\n",
      "You can now use this model for inference!\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "  Location: qlora_checkpoints/opt-1.3b-qlora-sst2/final_model\n",
      "\n",
      "Model files saved:\n",
      "  - LoRA adapters\n",
      "  - Tokenizer\n",
      "  - Training arguments\n",
      "  - Evaluation results (if available)\n",
      "\n",
      "You can now use this model for inference!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model locally\n",
    "print(\"Saving model locally...\")\n",
    "\n",
    "# Save to local output directory\n",
    "final_model_path = os.path.join(args.output_path, 'final_model')\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save arguments\n",
    "with open(os.path.join(final_model_path, 'arguments.txt'), 'w') as f:\n",
    "    for k, v in args.__dict__.items():\n",
    "        f.write(f'{k}: {v}\\n')\n",
    "\n",
    "# Save evaluation results if they exist\n",
    "import shutil\n",
    "eval_comparison_path = os.path.join(args.output_path, 'evaluation_comparison.pkl')\n",
    "if os.path.exists(eval_comparison_path):\n",
    "    shutil.copy(eval_comparison_path, os.path.join(final_model_path, 'evaluation_comparison.pkl'))\n",
    "    print(\"Evaluation comparison saved.\")\n",
    "\n",
    "print(f\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"  Location: {final_model_path}\")\n",
    "print(f\"\\nModel files saved:\")\n",
    "print(f\"  - LoRA adapters\")\n",
    "print(f\"  - Tokenizer\")\n",
    "print(f\"  - Training arguments\")\n",
    "print(f\"  - Evaluation results (if available)\")\n",
    "print(\"\\nYou can now use this model for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354dc84",
   "metadata": {
    "id": "c354dc84"
   },
   "source": [
    "## 15. Evaluate the Model (Optional)\n",
    "\n",
    "This cell evaluates the fine-tuned model on all 4 splits:\n",
    "- **Train Retain** - training data that should be retained\n",
    "- **Train Forget** - training data that should be \"forgotten\" (for unlearning experiments)\n",
    "- **Test Retain** - held-out test data for retained knowledge\n",
    "- **Test Forget** - held-out test data for forgotten knowledge\n",
    "\n",
    "The comparison section shows the improvement (Î”) from the base model to the fine-tuned model for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb338c24",
   "metadata": {
    "id": "bb338c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š Fine-tuned Model Evaluation Results:\n",
      "============================================================\n",
      "\n",
      "ðŸ”¹ Evaluating Train Retain (Fine-tuned Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Train Retain:   0%|          | 0/5792 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Train Retain: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5792/5792 [1:00:49<00:00,  1.59it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9661\n",
      "\n",
      "ðŸ”¹ Evaluating Train Forget (Fine-tuned Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Train Forget: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [01:50<00:00,  1.63it/s]\n",
      "Evaluating Train Forget: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179/179 [01:50<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9684\n",
      "\n",
      "ðŸ”¹ Evaluating Test Retain (Fine-tuned Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Retain: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2482/2482 [24:56<00:00,  1.66it/s]\n",
      "Evaluating Test Retain: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2482/2482 [24:56<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9526\n",
      "\n",
      "ðŸ”¹ Evaluating Test Forget (Fine-tuned Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test Forget: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:46<00:00,  1.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9492\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Comparison: Base vs Fine-tuned Model\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 106\u001b[0m\n\u001b[1;32m    102\u001b[0m     arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ†‘\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m diff \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ†“\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m diff \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ†’\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Accuracy: Base=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fine-tuned=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mft_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Î”=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m+.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff_pct\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m+.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m compare_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Retain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbaseline_results\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_retain\u001b[39m\u001b[38;5;124m'\u001b[39m], train_retain_finetuned)\n\u001b[1;32m    107\u001b[0m compare_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Forget\u001b[39m\u001b[38;5;124m\"\u001b[39m, baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_forget\u001b[39m\u001b[38;5;124m'\u001b[39m], train_forget_finetuned)\n\u001b[1;32m    108\u001b[0m compare_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Retain\u001b[39m\u001b[38;5;124m\"\u001b[39m, baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_retain\u001b[39m\u001b[38;5;124m'\u001b[39m], test_retain_finetuned)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_results' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on all 4 splits\n",
    "# Using manual evaluation (not trainer.evaluate) to avoid SFT data processing issues\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Fine-tuned Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_finetuned_model(model, dataset, tokenizer, batch_size=8, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate fine-tuned model using the same approach as base model evaluation.\n",
    "    Returns accuracy only (matching baseline format).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Map sentiment labels to expected tokens\n",
    "    sentiment_mapping = {\n",
    "        'positive': tokenizer.encode(' positive', add_special_tokens=False)[0],\n",
    "        'negative': tokenizer.encode(' negative', add_special_tokens=False)[0],\n",
    "        'Positive': tokenizer.encode(' Positive', add_special_tokens=False)[0],\n",
    "        'Negative': tokenizer.encode(' Negative', add_special_tokens=False)[0],\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=f\"Evaluating {split_name}\"):\n",
    "            batch_end = min(i + batch_size, len(dataset))\n",
    "            batch = dataset[i:batch_end]\n",
    "            \n",
    "            for example in batch['text']:\n",
    "                # Split into prompt and completion\n",
    "                if \"### Sentiment:\" in example:\n",
    "                    prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "                    true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "                    \n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Get model prediction\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    predicted_token = torch.argmax(next_token_logits).item()\n",
    "                    \n",
    "                    # Check if predicted token matches any sentiment token\n",
    "                    true_sentiment_lower = true_sentiment.lower().strip()\n",
    "                    \n",
    "                    # Get expected token(s)\n",
    "                    expected_tokens = []\n",
    "                    if 'positive' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('positive'), sentiment_mapping.get('Positive')]\n",
    "                    elif 'negative' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('negative'), sentiment_mapping.get('Negative')]\n",
    "                    \n",
    "                    expected_tokens = [t for t in expected_tokens if t is not None]\n",
    "                    \n",
    "                    if predicted_token in expected_tokens:\n",
    "                        correct += 1\n",
    "                    \n",
    "                    total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Evaluate on all 4 splits\n",
    "print(\"\\nðŸ”¹ Evaluating Train Retain (Fine-tuned Model)...\")\n",
    "train_retain_finetuned = evaluate_finetuned_model(model, dataset['train_retain'], tokenizer, batch_size=8, split_name=\"Train Retain\")\n",
    "print(f\"  Accuracy: {train_retain_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Forget (Fine-tuned Model)...\")\n",
    "train_forget_finetuned = evaluate_finetuned_model(model, dataset['train_forget'], tokenizer, batch_size=8, split_name=\"Train Forget\")\n",
    "print(f\"  Accuracy: {train_forget_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Retain (Fine-tuned Model)...\")\n",
    "test_retain_finetuned = evaluate_finetuned_model(model, dataset['test_retain'], tokenizer, batch_size=8, split_name=\"Test Retain\")\n",
    "print(f\"  Accuracy: {test_retain_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Forget (Fine-tuned Model)...\")\n",
    "test_forget_finetuned = evaluate_finetuned_model(model, dataset['test_forget'], tokenizer, batch_size=8, split_name=\"Test Forget\")\n",
    "print(f\"  Accuracy: {test_forget_finetuned['accuracy']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ac05a",
   "metadata": {
    "id": "d23ac05a"
   },
   "source": [
    "# ðŸ”„ Independent Inference Section\n",
    "\n",
    "**This section can be run independently after fine-tuning is complete!**\n",
    "\n",
    "Run these cells to:\n",
    "1. Load your saved fine-tuned model from local storage\n",
    "2. Load the saved dataset splits\n",
    "3. Evaluate the model on all 4 splits\n",
    "4. Compare results with baseline (if available)\n",
    "\n",
    "**Note:** Make sure you have already trained and saved the model (Section 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccebc8c",
   "metadata": {
    "id": "6ccebc8c"
   },
   "source": [
    "## Step 1: Configure Paths\n",
    "\n",
    "Set the paths to your saved model and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f16f3",
   "metadata": {
    "id": "b80f16f3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 1: Configure Paths\n",
    "Run this cell to set up paths to your saved model and datasets\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Configuration - MODIFY THESE TO MATCH YOUR SAVED MODEL\n",
    "BASE_MODEL_NAME = \"facebook/opt-1.3b\"  # Original base model\n",
    "DATASET_NAME = \"sst2\"  # Dataset used for training\n",
    "MODEL_CHECKPOINT_DIR = f'qlora_checkpoints/{BASE_MODEL_NAME.split(\"/\")[-1].lower()}-qlora-{DATASET_NAME.lower()}'\n",
    "\n",
    "# Paths\n",
    "SAVED_MODEL_PATH = os.path.join(MODEL_CHECKPOINT_DIR, 'final_model')\n",
    "DATASETS_PATH = os.path.join(MODEL_CHECKPOINT_DIR, 'datasets')\n",
    "BASELINE_PATH = os.path.join(MODEL_CHECKPOINT_DIR, 'evaluation_comparison.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”§ Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base Model: {BASE_MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model Path: {SAVED_MODEL_PATH}\")\n",
    "print(f\"Datasets Path: {DATASETS_PATH}\")\n",
    "print(f\"Baseline Results: {BASELINE_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print(f\"\\nâš ï¸  WARNING: Model path not found: {SAVED_MODEL_PATH}\")\n",
    "    print(\"   Make sure you have trained and saved the model first!\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Model found at: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "if not os.path.exists(DATASETS_PATH):\n",
    "    print(f\"\\nâš ï¸  WARNING: Datasets path not found: {DATASETS_PATH}\")\n",
    "    print(\"   You may need to load from HuggingFace instead.\")\n",
    "else:\n",
    "    print(f\"âœ… Datasets found at: {DATASETS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc36fb",
   "metadata": {
    "id": "f1bc36fb"
   },
   "source": [
    "## Step 2: Load Fine-tuned Model\n",
    "\n",
    "Load the saved fine-tuned model with LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1506cde",
   "metadata": {
    "id": "b1506cde"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 2: Load Fine-tuned Model\n",
    "This cell loads your saved fine-tuned model from local storage\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”„ Loading Fine-tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the base model with 4-bit quantization\n",
    "print(\"\\n1ï¸âƒ£ Loading base model with 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"   âœ… Base model loaded\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\n2ï¸âƒ£ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"   âœ… Tokenizer loaded\")\n",
    "\n",
    "# Load the fine-tuned LoRA adapters\n",
    "print(\"\\n3ï¸âƒ£ Loading fine-tuned LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, SAVED_MODEL_PATH)\n",
    "print(\"   âœ… LoRA adapters loaded\")\n",
    "\n",
    "# Move to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f75787",
   "metadata": {
    "id": "01f75787"
   },
   "source": [
    "## Step 3: Load Dataset Splits\n",
    "\n",
    "Load the saved dataset splits from local storage (or from HuggingFace if not available locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d778f",
   "metadata": {
    "id": "c20d778f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 3: Load Dataset Splits\n",
    "Load datasets from local storage or HuggingFace\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Loading Dataset Splits\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "# Try to load from local storage first\n",
    "if os.path.exists(DATASETS_PATH):\n",
    "    print(\"\\nðŸ”¹ Loading from local storage...\")\n",
    "    for split_name in ['train_retain', 'train_forget', 'test_retain', 'test_forget']:\n",
    "        split_path = os.path.join(DATASETS_PATH, split_name)\n",
    "        if os.path.exists(split_path):\n",
    "            dataset[split_name] = load_from_disk(split_path)\n",
    "            print(f\"   âœ… {split_name}: {len(dataset[split_name])} examples\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  {split_name} not found at {split_path}\")\n",
    "else:\n",
    "    # Load from HuggingFace if local not available\n",
    "    print(\"\\nðŸ”¹ Loading from HuggingFace...\")\n",
    "    data_path = \"karuna-bhaila/Unlearning_SST2v3\"\n",
    "    \n",
    "    def preprocess_sentiment(examples):\n",
    "        prompt = f\"\"\"### Text: {examples['text']}\\n\\n### Question: What is the sentiment of the given text?\\n\\n### Sentiment:\"\"\"\n",
    "        completion = f\"\"\" {examples['label_text']}\"\"\"\n",
    "        return {\"text\": prompt + completion}\n",
    "    \n",
    "    raw_dataset = load_dataset(data_path)\n",
    "    dataset = raw_dataset.map(preprocess_sentiment, batched=False)\n",
    "    dataset = dataset.remove_columns(['label', 'label_text'])\n",
    "    dataset.set_format(\"torch\")\n",
    "    \n",
    "    for split_name in dataset.keys():\n",
    "        print(f\"   âœ… {split_name}: {len(dataset[split_name])} examples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Datasets loaded successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c94e1f",
   "metadata": {
    "id": "a4c94e1f"
   },
   "source": [
    "## Step 4: Define Evaluation Function\n",
    "\n",
    "This function evaluates the model on a given dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3f5bf",
   "metadata": {
    "id": "3ee3f5bf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 4: Define Evaluation Function\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataset_split, tokenizer, batch_size=8, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model using single-token prediction approach.\n",
    "    Returns accuracy by checking if the model predicts the correct sentiment token.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Map sentiment labels to expected tokens\n",
    "    sentiment_mapping = {\n",
    "        'positive': tokenizer.encode(' positive', add_special_tokens=False)[0],\n",
    "        'negative': tokenizer.encode(' negative', add_special_tokens=False)[0],\n",
    "        'Positive': tokenizer.encode(' Positive', add_special_tokens=False)[0],\n",
    "        'Negative': tokenizer.encode(' Negative', add_special_tokens=False)[0],\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset_split), batch_size), desc=f\"Evaluating {split_name}\"):\n",
    "            batch_end = min(i + batch_size, len(dataset_split))\n",
    "            batch = dataset_split[i:batch_end]\n",
    "            \n",
    "            for example in batch['text']:\n",
    "                # Split into prompt and completion\n",
    "                if \"### Sentiment:\" in example:\n",
    "                    prompt_part = example.split(\"### Sentiment:\")[0] + \"### Sentiment:\"\n",
    "                    true_sentiment = example.split(\"### Sentiment:\")[1].strip()\n",
    "                    \n",
    "                    # Tokenize prompt\n",
    "                    inputs = tokenizer(prompt_part, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # Get model prediction\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    next_token_logits = logits[0, -1, :]\n",
    "                    predicted_token = torch.argmax(next_token_logits).item()\n",
    "                    \n",
    "                    # Check if predicted token matches any sentiment token\n",
    "                    true_sentiment_lower = true_sentiment.lower().strip()\n",
    "                    \n",
    "                    # Get expected token(s)\n",
    "                    expected_tokens = []\n",
    "                    if 'positive' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('positive'), sentiment_mapping.get('Positive')]\n",
    "                    elif 'negative' in true_sentiment_lower:\n",
    "                        expected_tokens = [sentiment_mapping.get('negative'), sentiment_mapping.get('Negative')]\n",
    "                    \n",
    "                    expected_tokens = [t for t in expected_tokens if t is not None]\n",
    "                    \n",
    "                    if predicted_token in expected_tokens:\n",
    "                        correct += 1\n",
    "                    \n",
    "                    total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return {'accuracy': accuracy, 'correct': correct, 'total': total}\n",
    "\n",
    "print(\"âœ… Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 7: Test with Custom Examples\n",
    "Try your own text samples\n",
    "\"\"\"\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        predicted_sentiment: The predicted sentiment label\n",
    "    \"\"\"\n",
    "    # Format prompt exactly as during training\n",
    "    prompt = f\"\"\"### Text: {text}\\n\\n### Question: What is the sentiment of the given text?\\n\\n### Sentiment:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        predicted_token = torch.argmax(next_token_logits).item()\n",
    "    \n",
    "    # Decode token\n",
    "    predicted_text = tokenizer.decode([predicted_token])\n",
    "    \n",
    "    # Map to sentiment\n",
    "    if 'positive' in predicted_text.lower():\n",
    "        return 'positive', predicted_text\n",
    "    elif 'negative' in predicted_text.lower():\n",
    "        return 'negative', predicted_text\n",
    "    else:\n",
    "        return 'unknown', predicted_text\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible experience. Would not recommend to anyone.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"Best purchase I've ever made! Highly recommended!\",\n",
    "    \"Waste of money. Very disappointed with the quality.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ¯ Testing with Custom Examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    sentiment, token = predict_sentiment(text, model, tokenizer)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted: {sentiment} (token: {token!r})\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nâœ… Custom testing complete!\")\n",
    "\n",
    "# Try your own text\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¡ To try your own text, modify and run:\")\n",
    "print(\"=\"*60)\n",
    "print('your_text = \"Your custom text here\"')\n",
    "print('sentiment, token = predict_sentiment(your_text, model, tokenizer)')\n",
    "print('print(f\"Predicted: {sentiment} (token: {token!r})\")')\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438aa96d",
   "metadata": {},
   "source": [
    "## Step 7: Test with Custom Examples (Optional)\n",
    "\n",
    "Try the model with your own text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2929180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 6: Compare with Baseline\n",
    "Load and compare with baseline results if available\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Try to load baseline results\n",
    "baseline_results = None\n",
    "if os.path.exists(BASELINE_PATH):\n",
    "    try:\n",
    "        with open(BASELINE_PATH, 'rb') as f:\n",
    "            comparison_data = pickle.load(f)\n",
    "            baseline_results = comparison_data.get('baseline', None)\n",
    "        print(\"âœ… Baseline results loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load baseline results: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No baseline results found. Skipping comparison.\")\n",
    "\n",
    "# Compare if baseline available\n",
    "if baseline_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“ˆ Comparison: Base vs Fine-tuned Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def compare_split(split_name, baseline, finetuned):\n",
    "        \"\"\"Compare baseline and fine-tuned results for a split.\"\"\"\n",
    "        print(f\"\\nðŸ”¹ {split_name} Comparison:\")\n",
    "        \n",
    "        base_acc = baseline.get('accuracy', 0.0)\n",
    "        ft_acc = finetuned.get('accuracy', 0.0)\n",
    "        diff = ft_acc - base_acc\n",
    "        diff_pct = (diff / base_acc * 100) if base_acc > 0 else float('inf')\n",
    "        arrow = \"â†‘\" if diff > 0 else \"â†“\" if diff < 0 else \"â†’\"\n",
    "        \n",
    "        print(f\"   Base Model:      {base_acc:.4f}\")\n",
    "        print(f\"   Fine-tuned:      {ft_acc:.4f}\")\n",
    "        print(f\"   Improvement:     {diff:+.4f} ({diff_pct:+.2f}%) {arrow}\")\n",
    "    \n",
    "    compare_split(\"Train Retain\", baseline_results['train_retain'], results['train_retain'])\n",
    "    compare_split(\"Train Forget\", baseline_results['train_forget'], results['train_forget'])\n",
    "    compare_split(\"Test Retain\", baseline_results['test_retain'], results['test_retain'])\n",
    "    compare_split(\"Test Forget\", baseline_results['test_forget'], results['test_forget'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Calculate average improvement\n",
    "    avg_base = sum(baseline_results[k]['accuracy'] for k in baseline_results.keys()) / len(baseline_results)\n",
    "    avg_ft = sum(results[k]['accuracy'] for k in results.keys()) / len(results)\n",
    "    avg_improvement = avg_ft - avg_base\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall Statistics:\")\n",
    "    print(f\"   Average Base Accuracy:       {avg_base:.4f}\")\n",
    "    print(f\"   Average Fine-tuned Accuracy: {avg_ft:.4f}\")\n",
    "    print(f\"   Average Improvement:         {avg_improvement:+.4f} ({(avg_improvement/avg_base*100):+.2f}%)\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\nðŸ’¡ Tip: Run the base model evaluation (Section 8.7) before training to generate baseline results for comparison.\")\n",
    "\n",
    "# Save current results\n",
    "save_path = os.path.join(MODEL_CHECKPOINT_DIR, 'inference_results.pkl')\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results': results,\n",
    "        'baseline': baseline_results,\n",
    "        'model_path': SAVED_MODEL_PATH,\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d3366",
   "metadata": {},
   "source": [
    "## Step 6: Compare with Baseline (Optional)\n",
    "\n",
    "If baseline results are available, compare the fine-tuned model with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b63601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INDEPENDENT INFERENCE - STEP 5: Evaluate on All Splits\n",
    "Run evaluation on all 4 dataset splits\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Fine-tuned Model Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on all 4 splits\n",
    "results = {}\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Retain...\")\n",
    "results['train_retain'] = evaluate_model(model, dataset['train_retain'], tokenizer, batch_size=8, split_name=\"Train Retain\")\n",
    "print(f\"   Accuracy: {results['train_retain']['accuracy']:.4f} ({results['train_retain']['correct']}/{results['train_retain']['total']})\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Train Forget...\")\n",
    "results['train_forget'] = evaluate_model(model, dataset['train_forget'], tokenizer, batch_size=8, split_name=\"Train Forget\")\n",
    "print(f\"   Accuracy: {results['train_forget']['accuracy']:.4f} ({results['train_forget']['correct']}/{results['train_forget']['total']})\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Retain...\")\n",
    "results['test_retain'] = evaluate_model(model, dataset['test_retain'], tokenizer, batch_size=8, split_name=\"Test Retain\")\n",
    "print(f\"   Accuracy: {results['test_retain']['accuracy']:.4f} ({results['test_retain']['correct']}/{results['test_retain']['total']})\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Evaluating Test Forget...\")\n",
    "results['test_forget'] = evaluate_model(model, dataset['test_forget'], tokenizer, batch_size=8, split_name=\"Test Forget\")\n",
    "print(f\"   Accuracy: {results['test_forget']['accuracy']:.4f} ({results['test_forget']['correct']}/{results['test_forget']['total']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Evaluation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5039a",
   "metadata": {
    "id": "0bd5039a"
   },
   "source": [
    "## Notes and Tips\n",
    "\n",
    "### Important: Modern TRL Approach (TRL 0.8.6+)\n",
    "This notebook uses the **modern TRL approach** with:\n",
    "- `SFTConfig` instead of `TrainingArguments`\n",
    "- `completion_only_loss=True` to mask prompt tokens\n",
    "- No need for `DataCollatorForCompletionOnlyLM` (deprecated/removed)\n",
    "\n",
    "This ensures only the sentiment label tokens contribute to the loss, not the prompt text.\n",
    "\n",
    "### Evaluation Strategy\n",
    "The notebook now includes:\n",
    "1. **Base Model Evaluation**: Evaluates the model before fine-tuning to establish a baseline\n",
    "2. **Fine-tuned Model Evaluation**: Evaluates after training on held-out test sets\n",
    "3. **Comparison Metrics**: Shows improvement (or degradation) across accuracy, F1-score, precision, and recall\n",
    "4. **Independent Inference**: Load model from Google Drive for standalone inference\n",
    "\n",
    "### GPU Requirements:\n",
    "- **Llama-2-7B**: Requires at least 12GB GPU memory (use Colab Pro with A100)\n",
    "- **OPT-1.3B**: Can run on free Colab with T4 GPU\n",
    "- **Llama-2-13B**: Requires 16GB+ GPU memory\n",
    "\n",
    "### Memory Optimization:\n",
    "- Reduce `train_batch_size` if you run out of memory\n",
    "- Reduce `max_length` to 512 or 256 for smaller memory footprint\n",
    "- Use gradient accumulation by adding `gradient_accumulation_steps=4` to `SFTConfig`\n",
    "\n",
    "### Training Tips:\n",
    "- Start with fewer epochs (3-5) for initial experimentation\n",
    "- Monitor the training loss and adjust learning rate if needed\n",
    "- Use W&B to track experiments and compare different configurations\n",
    "- The evaluation comparison shows if fine-tuning improved the model\n",
    "\n",
    "### Inference Tips:\n",
    "- The inference section (16) is completely independent\n",
    "- You can run it in a new notebook session by copying just those cells\n",
    "- Model is saved in Google Drive for persistence across sessions\n",
    "- Adjust `max_new_tokens` and `temperature` for different generation behaviors\n",
    "\n",
    "### TRL Version Compatibility:\n",
    "- This notebook is designed for TRL 0.8.6+\n",
    "- If using older TRL versions (<0.8), you may need to use `DataCollatorForCompletionOnlyLM`\n",
    "- For newest TRL versions (0.19+), ensure `completion_only_loss` is available in `SFTConfig`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "75c8f0fb",
    "9604e99e",
    "ea22656a",
    "376c9a99",
    "686b6ce3",
    "c354dc84",
    "d23ac05a"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01621312971243d7a67d32d36d658843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01cba15091c04694b3a36c6de842e7bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0248b5a3abe241e090d28906a2b2a9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3166bac99cf4dd292b1c6abc397d9c6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1b081285ed6c4aa2a4694fc22fc87fcb",
      "value": "Truncatingâ€‡trainâ€‡dataset:â€‡100%"
     }
    },
    "0402af89bb794e6eb7555cad1d901c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "042931cf1a444cbb9579d6deed3fed10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0634e995e6324f549f625747cd8d8314": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d65a797c050424db9387234eee88d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e87a70ff1a494cbeb06b9832d4ff04dc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a577589f67e2428189a38d2367b61802",
      "value": "Truncatingâ€‡testâ€‡dataset:â€‡100%"
     }
    },
    "0d6bf51bf71848989bad1ffec27da867": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dc181010b494c8a9e32431d9ec0698a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f048e7e492049bcbc19c726df579f06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f7de5d5f49d499ab0acbd2912253e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e9123a20d394d71a0f80a857414f585",
      "max": 47756,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5d12fb9969f41c9b6efe2fef8aeb826",
      "value": 47756
     }
    },
    "105d099eb38a42e3bb4b75099616257e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "127583a2e61b44d99d9758deba55c255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_449b6adeaa24431da00df5983beae5d4",
      "max": 685,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b757a791e8e140b79cfdc6417808cefb",
      "value": 685
     }
    },
    "167f8cd6ffa943d99cbd98fa6bc601c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "196a8b4f13c746fdacd34a4cd640f678": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b081285ed6c4aa2a4694fc22fc87fcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bf98c860f3249c18122dfa42c727b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_314afd5d344c44efbf413ea46b87f43a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aab24403c4df4c1aa836638f623f123c",
      "value": "Tokenizingâ€‡trainâ€‡dataset:â€‡100%"
     }
    },
    "1c08fb3d10b04f8cbeebaca21feeed4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d1c683dcafa45a7852ba89a693bb79a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f82efa17b2f44a3a460677d15638494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f10882d6a9924b08ab7b4b026de0c654",
       "IPY_MODEL_f894663625a04d53a4ac03f955a80aa2",
       "IPY_MODEL_cc8566190997446a90aa915540be02b4"
      ],
      "layout": "IPY_MODEL_f9226ee898a34b6099bacdb0ded5af57"
     }
    },
    "206deb29b13d4b92b44dba72150127b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2088e16a2e9f4954a247fdd34b13ba40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bf98c860f3249c18122dfa42c727b40",
       "IPY_MODEL_0f7de5d5f49d499ab0acbd2912253e4f",
       "IPY_MODEL_f52e60a527e5440d92f7823244c6b1ea"
      ],
      "layout": "IPY_MODEL_3e27ea48f3a943548ec6b2a8d5689872"
     }
    },
    "21cb286735e748e5a2f83a76fdb2dc7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f77cbd70673149b2b0a4fe3674e47a54",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_44eec3eeb3f34d74820ab9d15f1c7f2e",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "222e3350654945f7baefc2da08dc83e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b22f7f99d0b43d68fd6ed4ff2055f00",
       "IPY_MODEL_fbced2f5a8304d7391de524599a5ed55",
       "IPY_MODEL_c6984bc2d6d54d4c96e240b8797a580a"
      ],
      "layout": "IPY_MODEL_fca858d3f4934d6cb3dc7fc5220b9bd0"
     }
    },
    "2298c627d666457f90c05783c322c6bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ffd1708f354abba28e28cd19b32da2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b720bc22e404ed584782bd5065c970e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21cb286735e748e5a2f83a76fdb2dc7c",
       "IPY_MODEL_4abcee57df474c39903b51387a1a12dc",
       "IPY_MODEL_8aea779ea0364928b8d44ef776eceb8e"
      ],
      "layout": "IPY_MODEL_c018f01293fd47b196bac238a848a936"
     }
    },
    "2d22e2f328724b628d0c53c45a4c3c11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60a98e621fb0479ebf4a8fdd6022cc7d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_80aaf89b211642a3b1f17152105acacf",
      "value": "â€‡20465/20465â€‡[00:06&lt;00:00,â€‡3915.87â€‡examples/s]"
     }
    },
    "314afd5d344c44efbf413ea46b87f43a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33001eb45df94747b2ebeed8714faa46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "347469a1ebec4972b8a4a0ae4b6da99c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0634e995e6324f549f625747cd8d8314",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_700ba68bc47b4a368e3dfa90d1164d06",
      "value": "â€‡47756/47756â€‡[00:00&lt;00:00,â€‡658134.33â€‡examples/s]"
     }
    },
    "36d62e948a8446fd85c1a83c0592e35f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33001eb45df94747b2ebeed8714faa46",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1d1c683dcafa45a7852ba89a693bb79a",
      "value": "config.json:â€‡100%"
     }
    },
    "392384f5182f4d7e99687aef9b5945dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9c1e50faa1041b19873a233986d3450",
      "max": 441,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0402af89bb794e6eb7555cad1d901c10",
      "value": 441
     }
    },
    "3a677e68d4734c5dbdb91ea5295a3c95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e211c1a5318470c8b1a1d9b42f640c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6ba9ad85d7f4889bb520c141dbe4b75",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_51f71617953e46fd8a33111dc5694508",
      "value": "â€‡441/441â€‡[00:00&lt;00:00,â€‡48.5kB/s]"
     }
    },
    "3e27ea48f3a943548ec6b2a8d5689872": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e398bbb4f4d46a995541c8d5a88bc03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e780ff9b8a54452bda0a86c10f35361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f73aefea2c24c5e9ade589c9d0f8266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40c709e561be452cb13775105c56f05f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01cba15091c04694b3a36c6de842e7bf",
      "value": "â€‡653/653â€‡[00:00&lt;00:00,â€‡19.5kB/s]"
     }
    },
    "408a2680e3584f14accbb8cc1720d1d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40c709e561be452cb13775105c56f05f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40cc37fe166d4f01accb396024bbe8a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42ca20be524d405b9bdf664c2abe48fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5161bcd721c4dd59290f09ef2a9c9ab",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c3b19d2536604671a5415e558447f4e9",
      "value": 1
     }
    },
    "449b6adeaa24431da00df5983beae5d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44eec3eeb3f34d74820ab9d15f1c7f2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "473c2c784a41414dbb9bf9a27fa14319": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a7b7da4e4e84b08ab55228617028c96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4abcee57df474c39903b51387a1a12dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fe1d484144540bd8711cab8f7b5c4fc",
      "max": 137,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_105d099eb38a42e3bb4b75099616257e",
      "value": 137
     }
    },
    "4fe1d484144540bd8711cab8f7b5c4fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5002c02e030b4a4d8aebceb160b0690d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51f71617953e46fd8a33111dc5694508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "556f6b71d0354a4d9f6850393e237d69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5916a861f3f74f9dad9078106f075dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_642e6d62f22a44309c78d500e8e753b8",
      "max": 47756,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_670057dcf38b4065aea234536473541c",
      "value": 47756
     }
    },
    "5a3f4c3e56ab4364a716690f986013f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b22f7f99d0b43d68fd6ed4ff2055f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b4a7e658e244c1f8938e0b0331fa851",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_40cc37fe166d4f01accb396024bbe8a6",
      "value": "Addingâ€‡EOSâ€‡toâ€‡testâ€‡dataset:â€‡100%"
     }
    },
    "5b4a7e658e244c1f8938e0b0331fa851": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60a98e621fb0479ebf4a8fdd6022cc7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "636a2d8a037e4f27aafd2af15eec57f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74a25c50c4514456ba7c1f99cf7778e1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3e398bbb4f4d46a995541c8d5a88bc03",
      "value": "â€‡456k/?â€‡[00:00&lt;00:00,â€‡23.2MB/s]"
     }
    },
    "642bbd550ff94abca438f53df2f66af0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e780ff9b8a54452bda0a86c10f35361",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01621312971243d7a67d32d36d658843",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "642e6d62f22a44309c78d500e8e753b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64f85e6894b14696bbe7bf32dd0a8216": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66041a6d4c294c999a31fb1179d45f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb3c306adab14151b479f83f54715c81",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ea4669e0310441bf806bf4fb65099ab5",
      "value": "â€‡20465/20465â€‡[00:00&lt;00:00,â€‡375331.47â€‡examples/s]"
     }
    },
    "666d04caf1dd4d0096682e57dd92c4aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aad9c231afd443d5b42d5828b78dc967",
       "IPY_MODEL_bfadf2374a6b4d4da831cd169f3a1991",
       "IPY_MODEL_2d22e2f328724b628d0c53c45a4c3c11"
      ],
      "layout": "IPY_MODEL_28ffd1708f354abba28e28cd19b32da2"
     }
    },
    "66f6dd3271274daaadd463e8b6b12c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0248b5a3abe241e090d28906a2b2a9f7",
       "IPY_MODEL_5916a861f3f74f9dad9078106f075dcd",
       "IPY_MODEL_347469a1ebec4972b8a4a0ae4b6da99c"
      ],
      "layout": "IPY_MODEL_b94078469166431ab9ba71dedc1cbcb6"
     }
    },
    "670057dcf38b4065aea234536473541c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c0eb0f90af443a482f2acb79a662cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa2f09ed37ed4f969a9296209a37f0b4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bb4cf1d6d6a48d89d7cde0b2e12129a",
      "value": 1
     }
    },
    "6c523fbcc928421698634c96b8b2b9ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e9123a20d394d71a0f80a857414f585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "700ba68bc47b4a368e3dfa90d1164d06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74a25c50c4514456ba7c1f99cf7778e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb4cf1d6d6a48d89d7cde0b2e12129a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e30461f3fe24e12afe8140ac815210a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ebeca4cf8744c70be4a93f129c560fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dc181010b494c8a9e32431d9ec0698a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0d6bf51bf71848989bad1ffec27da867",
      "value": "â€‡47756/47756â€‡[00:01&lt;00:00,â€‡32938.16â€‡examples/s]"
     }
    },
    "8021cb2abe20402cb515281e8a4a381d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80aaf89b211642a3b1f17152105acacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "830e3167950c45a395305a730972726f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6b996d365e14e5daca94db10fff63a1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1c08fb3d10b04f8cbeebaca21feeed4f",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "86b79c4437a04202aa1be996d932b229": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8793b9619aad4e2c8e0b99e63e90eb31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8948b830963547e9b4aa91dc84ca9e6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8aea779ea0364928b8d44ef776eceb8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc235e58e59c49129ac9195857d62b40",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e89d098e1fd244ddb6e06f6861645549",
      "value": "â€‡137/137â€‡[00:00&lt;00:00,â€‡13.2kB/s]"
     }
    },
    "8cedd30b562147a5b812d75f9156114a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90cbbe8565414848a9e9e82984326a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4f1ee0381f946fba67b3e656b314a70",
      "max": 47756,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8793b9619aad4e2c8e0b99e63e90eb31",
      "value": 47756
     }
    },
    "9a0dab9c7b2d432c93561e34682fb0b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aaec66d90194901ac0e5452bec632e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9dd077cd7b8e4a61bf9d9700a6cec680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4f1ee0381f946fba67b3e656b314a70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a577589f67e2428189a38d2367b61802": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6b996d365e14e5daca94db10fff63a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ba9ad85d7f4889bb520c141dbe4b75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7a79853af574023b20339ec5382ea71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa2f09ed37ed4f969a9296209a37f0b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "aab24403c4df4c1aa836638f623f123c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aad9c231afd443d5b42d5828b78dc967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5002c02e030b4a4d8aebceb160b0690d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c4a63c134c704906bb6cf83abb6bf983",
      "value": "Tokenizingâ€‡testâ€‡dataset:â€‡100%"
     }
    },
    "ab3abdba0f3f4a09a5530a5071f460e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ac7bf77a17674a27bf231914a4dca3c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9d4c5fa314949b084da17d79b36d04e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_473c2c784a41414dbb9bf9a27fa14319",
      "value": "â€‡899k/?â€‡[00:00&lt;00:00,â€‡11.2MB/s]"
     }
    },
    "acc053353ed1405f9722ef645f984c67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36d62e948a8446fd85c1a83c0592e35f",
       "IPY_MODEL_edea48a1ca254d88aafbbf2b75562835",
       "IPY_MODEL_3f73aefea2c24c5e9ade589c9d0f8266"
      ],
      "layout": "IPY_MODEL_9a0dab9c7b2d432c93561e34682fb0b4"
     }
    },
    "b02fe2e9014f4668af26b5e7d0c85163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a3f4c3e56ab4364a716690f986013f7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d2ee4bd1e31a4cb4ab3d57d9f3880164",
      "value": "Addingâ€‡EOSâ€‡toâ€‡trainâ€‡dataset:â€‡100%"
     }
    },
    "b2a2dd6c702f4e7097cf62ac43c5a738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b695524af4c644ec943a828ac7d6d68e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b757a791e8e140b79cfdc6417808cefb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b94078469166431ab9ba71dedc1cbcb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9ff68d77d3c4d4c804528c1ebf6bb4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_830e3167950c45a395305a730972726f",
       "IPY_MODEL_127583a2e61b44d99d9758deba55c255",
       "IPY_MODEL_efdb58c4d77c4a4b9e334a592fb4a9ce"
      ],
      "layout": "IPY_MODEL_206deb29b13d4b92b44dba72150127b8"
     }
    },
    "bfadf2374a6b4d4da831cd169f3a1991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7a79853af574023b20339ec5382ea71",
      "max": 20465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab3abdba0f3f4a09a5530a5071f460e3",
      "value": 20465
     }
    },
    "c018f01293fd47b196bac238a848a936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0e9833bc49540a8b93cde75599ed5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d65a797c050424db9387234eee88d4a",
       "IPY_MODEL_e44195cfdd734eb69e522bece242014b",
       "IPY_MODEL_66041a6d4c294c999a31fb1179d45f7c"
      ],
      "layout": "IPY_MODEL_64f85e6894b14696bbe7bf32dd0a8216"
     }
    },
    "c3b19d2536604671a5415e558447f4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4a63c134c704906bb6cf83abb6bf983": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4bd8b30a256465d9eca61cca5d0d296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5161bcd721c4dd59290f09ef2a9c9ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c6984bc2d6d54d4c96e240b8797a580a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff3278b49ea24ca98edf16b99d0966c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ceb8c17d8d0c480683031b60ff405730",
      "value": "â€‡20465/20465â€‡[00:00&lt;00:00,â€‡32309.91â€‡examples/s]"
     }
    },
    "c9d4c5fa314949b084da17d79b36d04e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ed6db3c9e14e64980c80dac42e3eac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f048e7e492049bcbc19c726df579f06",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_196a8b4f13c746fdacd34a4cd640f678",
      "value": "merges.txt:â€‡"
     }
    },
    "cc235e58e59c49129ac9195857d62b40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc8566190997446a90aa915540be02b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a7b7da4e4e84b08ab55228617028c96",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9aaec66d90194901ac0e5452bec632e6",
      "value": "â€‡2.63G/2.63Gâ€‡[00:24&lt;00:00,â€‡75.6MB/s]"
     }
    },
    "ceb8c17d8d0c480683031b60ff405730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2ee4bd1e31a4cb4ab3d57d9f3880164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3166bac99cf4dd292b1c6abc397d9c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d41044eff63f42ffb961f4fae1201a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_642bbd550ff94abca438f53df2f66af0",
       "IPY_MODEL_392384f5182f4d7e99687aef9b5945dc",
       "IPY_MODEL_3e211c1a5318470c8b1a1d9b42f640c8"
      ],
      "layout": "IPY_MODEL_2298c627d666457f90c05783c322c6bf"
     }
    },
    "d5d12fb9969f41c9b6efe2fef8aeb826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8c64c3d6fe64907b5e3d880b3a13477": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df485fb64515438eb6f8ff0ab91f448e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e44195cfdd734eb69e522bece242014b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c523fbcc928421698634c96b8b2b9ba",
      "max": 20465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_408a2680e3584f14accbb8cc1720d1d3",
      "value": 20465
     }
    },
    "e642108b6b5b46f489e4795a006da188": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b02fe2e9014f4668af26b5e7d0c85163",
       "IPY_MODEL_90cbbe8565414848a9e9e82984326a51",
       "IPY_MODEL_7ebeca4cf8744c70be4a93f129c560fa"
      ],
      "layout": "IPY_MODEL_556f6b71d0354a4d9f6850393e237d69"
     }
    },
    "e87a70ff1a494cbeb06b9832d4ff04dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e89d098e1fd244ddb6e06f6861645549": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9c1e50faa1041b19873a233986d3450": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4669e0310441bf806bf4fb65099ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb3c306adab14151b479f83f54715c81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed8fdecbd7004d58a0f77f9c77e9096a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9ed6db3c9e14e64980c80dac42e3eac",
       "IPY_MODEL_42ca20be524d405b9bdf664c2abe48fa",
       "IPY_MODEL_636a2d8a037e4f27aafd2af15eec57f5"
      ],
      "layout": "IPY_MODEL_86b79c4437a04202aa1be996d932b229"
     }
    },
    "edea48a1ca254d88aafbbf2b75562835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9db40114c014f78b7054a35685d9b34",
      "max": 653,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2a2dd6c702f4e7097cf62ac43c5a738",
      "value": 653
     }
    },
    "efdb58c4d77c4a4b9e334a592fb4a9ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e30461f3fe24e12afe8140ac815210a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8cedd30b562147a5b812d75f9156114a",
      "value": "â€‡685/685â€‡[00:00&lt;00:00,â€‡56.6kB/s]"
     }
    },
    "f060a094ac464f85ae0c601706b244f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f06e52e5197f49128d8c276b79581dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a677e68d4734c5dbdb91ea5295a3c95",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_042931cf1a444cbb9579d6deed3fed10",
      "value": "vocab.json:â€‡"
     }
    },
    "f10882d6a9924b08ab7b4b026de0c654": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_167f8cd6ffa943d99cbd98fa6bc601c8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_df485fb64515438eb6f8ff0ab91f448e",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "f39b7951fd7a48749fee874eccf8074a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f06e52e5197f49128d8c276b79581dc4",
       "IPY_MODEL_6c0eb0f90af443a482f2acb79a662cb0",
       "IPY_MODEL_ac7bf77a17674a27bf231914a4dca3c2"
      ],
      "layout": "IPY_MODEL_8021cb2abe20402cb515281e8a4a381d"
     }
    },
    "f52e60a527e5440d92f7823244c6b1ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4bd8b30a256465d9eca61cca5d0d296",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8948b830963547e9b4aa91dc84ca9e6a",
      "value": "â€‡47756/47756â€‡[00:12&lt;00:00,â€‡3804.88â€‡examples/s]"
     }
    },
    "f77cbd70673149b2b0a4fe3674e47a54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f894663625a04d53a4ac03f955a80aa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b695524af4c644ec943a828ac7d6d68e",
      "max": 2631561680,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f060a094ac464f85ae0c601706b244f1",
      "value": 2631561680
     }
    },
    "f9226ee898a34b6099bacdb0ded5af57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9db40114c014f78b7054a35685d9b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbced2f5a8304d7391de524599a5ed55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dd077cd7b8e4a61bf9d9700a6cec680",
      "max": 20465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8c64c3d6fe64907b5e3d880b3a13477",
      "value": 20465
     }
    },
    "fca858d3f4934d6cb3dc7fc5220b9bd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3278b49ea24ca98edf16b99d0966c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
